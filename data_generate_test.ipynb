{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/hyt/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList, TextIteratorStreamer\n",
    "from threading import Thread\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0,1,2,3,4,5,6,7'\n",
    "import torch.nn as nn\n",
    "import inspect\n",
    "import torch.multiprocessing as mp\n",
    "# mp.set_start_method('spawn')\n",
    "import json, pickle, copy\n",
    "from peft import PeftModel\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.73it/s]\n"
     ]
    }
   ],
   "source": [
    "# model_pth = '../Llama-2-7b-chat-hf'\n",
    "# peft_model_pth = './llama_2_7b_lora_3/5_epoch_finetuning'\n",
    "model_pth = '../Meta-Llama-3-8B-Instruct'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_pth,torch_dtype=torch.float16,load_in_4bit=False,load_in_8bit=False)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_pth,torch_dtype=torch.float16,load_in_4bit=False,load_in_8bit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        stop_ids = [2]  # IDs of tokens where the generation should stop.\n",
    "        for stop_id in stop_ids:\n",
    "            if input_ids[0][-1] == stop_id:  # Checking if the last generated token is a stop token.\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "class BatchedStopOnTokens(StoppingCriteria):\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        stop_ids = [2]  # IDs of tokens where the generation should stop.\n",
    "        for stop_id in stop_ids:\n",
    "            # checking if stop token appears in every batch (not just the last token)\n",
    "            if (input_ids == stop_id).any(dim=-1).all():\n",
    "                return True\n",
    "            # check if stop token is generated in all batches\n",
    "            # if all([input_id[-1] == stop_id for input_id in input_ids]):\n",
    "            #     return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/grade_school_math/data/test.jsonl', 'r') as f:\n",
    "    data_test = f.readlines()\n",
    "    data_test = [json.loads(d) for d in data_test]\n",
    "\n",
    "with open('../data/grade_school_math/data/train.jsonl', 'r') as f:\n",
    "    data_train = f.readlines()\n",
    "    data_train = [json.loads(d) for d in data_train]\n",
    "\n",
    "with open('./generated_data/actor_response_data_01.jsonl', 'r') as f:\n",
    "    actor_response_data = f.readlines()\n",
    "    actor_response_data = [json.loads(d) for d in actor_response_data]\n",
    "\n",
    "with open('./generated_data/critic_response_data_01.jsonl', 'r') as f:\n",
    "    critic_response_data = f.readlines()\n",
    "    critic_response_data = [json.loads(d) for d in critic_response_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "models = [copy.deepcopy(model).to(torch.device(f'cuda:{_}')) for _ in range(8)]\n",
    "tokenizers = [AutoTokenizer.from_pretrained(model_pth,torch_dtype=torch.float16,load_in_4bit=False,load_in_8bit=False) for _ in range(8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actor_models = [PeftModel.from_pretrained(models[i], peft_model_pth, torch_dtype=torch.float16) for i in range(8)]\n",
    "# critic_models = [PeftModel.from_pretrained(models[i], peft_model_pth, torch_dtype=torch.float16) for i in range(8)]\n",
    "# summarizer_models = [PeftModel.from_pretrained(models[i], peft_model_pth, torch_dtype=torch.float16) for i in range(8)]\n",
    "\n",
    "# actor_models = [PeftModel.from_pretrained(models[i], peft_model_pth, torch_dtype=torch.float16) for i in range(4)]\n",
    "critic_models = models\n",
    "summarizer_models = models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7473 1319 133632 133632\n"
     ]
    }
   ],
   "source": [
    "# id = 0\n",
    "# for data in data_train + data_test:\n",
    "#     data['id'] = id\n",
    "#     id += 1\n",
    "print(len(data_train), len(data_test), len(actor_response_data), len(critic_response_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71477 0 62155\n"
     ]
    }
   ],
   "source": [
    "ac, wa, iv = 0, 0, 0\n",
    "for critic_response in critic_response_data:\n",
    "    if critic_response['judge_critic'] == 'Accepted':\n",
    "        ac += 1\n",
    "    elif critic_response['judge_critic'] == 'Wrong Answer':\n",
    "        wa += 1\n",
    "    else:\n",
    "        iv += 1\n",
    "print(ac, wa, iv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### The answer is: Accepted.\n",
      "Accepted\n",
      "#### Wrong Answer: The actor added the extra 2 gallons for the bleach cycle to the total amount of water already calculated, which is incorrect. The correct approach would be to add the extra 2 gallons to the total amount of water needed, which is already 72 gallons, resulting in a total of 74 gallons.\n",
      "Invalid\n",
      "#### The answer is: Accepted.\n",
      "Accepted\n",
      "#### Wrong Answer: The actor's answer does not take into account the correct calculation of the amount Ian paid to Helen and Benedict.\n",
      "Invalid\n",
      "#### The answer is: Accepted.\n",
      "Accepted\n",
      "#### The answer is: Accepted.\n",
      "Accepted\n",
      "#### The answer is: Accepted.\n",
      "Accepted\n",
      "#### The answer is: Accepted.\n",
      "Accepted\n",
      "#### Wrong Answer: The calculation is correct, but the answer is wrong because the actor's answer is not in line with the problem. The problem asks for the age of the fourth child, not the year of birth.\n",
      "Invalid\n",
      "#### Wrong Answer: The actor's calculation for the total number of push-ups is incorrect.\n",
      "Invalid\n",
      "#### Wrong Answer: The calculation is correct up to the point where it says $0.50 / $0.05 = 10. However, the student forgot to round down the result to the nearest whole number, as you cannot give a fraction of a nickel as change. The correct answer is 9 nickels.\n",
      "Invalid\n",
      "#### The answer is: Accepted.\n",
      "Accepted\n",
      "#### Wrong Answer: The actor correctly calculated Jake's initial expenses, but incorrectly calculated the amount spent on the concert ticket.\n",
      "Invalid\n",
      "#### Wrong Answer: The actor's answer is incorrect because they are adding the eggs collected by the six egg hunters in the two rounds together, instead of considering the fact that the problem only provides the total number of eggs collected by the six egg hunters in the second round.\n",
      "Invalid\n",
      "#### Wrong Answer: The actor's answer seems to be a combination of correct steps and incorrect calculations. They correctly found the total amount of money earned from selling brownies and lemon squares, but then incorrectly calculated the amount of money left to earn and the number of cookies needed to sell.\n",
      "Invalid\n",
      "#### Wrong Answer: The calculation for the total amount of money Frank has is incorrect.\n",
      "Invalid\n"
     ]
    }
   ],
   "source": [
    "for critic_response in critic_response_data[:16]:\n",
    "    print(critic_response['critic_response'])\n",
    "    print(critic_response['judge_critic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tokenizer in tokenizers:\n",
    "    tokenizer.pad_token = tokenizer.bos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS_RE = re.compile(r\"#### (\\-?[0-9\\.\\,]+)\")\n",
    "def extract_answer(completion):\n",
    "    match = ANS_RE.search(completion)\n",
    "    if match:\n",
    "        match_str = match.group(1).strip()\n",
    "        match_str = match_str.replace(\",\", \"\")\n",
    "        return match_str\n",
    "    else:\n",
    "        return \"[invalid]\"    \n",
    "\n",
    "def extract_judgement(output):\n",
    "    if output.startswith(\"#### The answer is: Accepted.\"):\n",
    "        return 'Accepted'\n",
    "    if output.startswith(\"#### The answer is: Wrong Answer.\"):\n",
    "        return 'Wrong Answer'\n",
    "    return 'Invalid'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_use = data_train[:16]\n",
    "def data_generate_e2e(actor, critic, summarizer, tokenizer, sys_prompts, data, batch_size, device, use_tqdm=False):\n",
    "    n_data = len(data)\n",
    "    n_batches = n_data // batch_size\n",
    "    results = []\n",
    "    stop = BatchedStopOnTokens()\n",
    "    generate_kwargs = dict(\n",
    "        inputs_embeds=None,\n",
    "        max_new_tokens=1024,\n",
    "        do_sample=True,\n",
    "        top_p=0.95,\n",
    "        top_k=50,\n",
    "        temperature=0.7,\n",
    "        num_beams=1,\n",
    "        stopping_criteria=StoppingCriteriaList([stop]),\n",
    "        attention_mask=None\n",
    "    )\n",
    "    if use_tqdm:\n",
    "        pbar = tqdm(total=n_data, desc=f'Data generating for actor response on {device}', ncols=100)\n",
    "    for batch_idx in range(n_batches):\n",
    "        batch_data = data[batch_idx*batch_size:(batch_idx+1)*batch_size]\n",
    "        questions = [d['question'] for d in batch_data]\n",
    "        answers = [d['answer'] for d in batch_data]\n",
    "        ids = [d['id'] for d in batch_data]\n",
    "\n",
    "        # generate batched input embeddings, attention mask for actor and apply to generate_kwargs\n",
    "        input_prompts = ['<|system|>:' + sys_prompts['actor'] + '</s>\\n<|user|>:' + question + '</s>\\n<|assistant|>:' for question in questions]\n",
    "        input_embeds = [actor.get_input_embeddings()(tokenizer(input_prompt, return_tensors='pt', padding=False, truncation=True, max_length=512).input_ids.to(device)) for input_prompt in input_prompts]\n",
    "        max_len = max([input_embed.size(1) for input_embed in input_embeds])\n",
    "        attention_mask = torch.concatenate([torch.cat([torch.zeros(max_len - input_embed.size(1), device=device), torch.ones(input_embed.size(1), device=device)]).unsqueeze(0) for input_embed in input_embeds], dim=0).to(dtype=torch.float16)\n",
    "        input_embeds = torch.concatenate([torch.cat([torch.zeros(1, max_len - input_embed.size(1), embedding_dim, device=device), input_embed], dim=1) for input_embed in input_embeds], dim=0).to(dtype=torch.float16)\n",
    "        generate_kwargs['inputs_embeds'] = input_embeds\n",
    "        generate_kwargs['attention_mask'] = attention_mask\n",
    "\n",
    "        # generate actor responses\n",
    "        outputs = actor.generate(**generate_kwargs)\n",
    "        for actor_output in outputs:\n",
    "            if tokenizer.eos_token_id in actor_output:\n",
    "                eos_idx = (actor_output == tokenizer.eos_token_id).nonzero()[0].item()\n",
    "                actor_output[eos_idx+1:] = tokenizer.pad_token_id\n",
    "        actor_responses = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "        # # generate batched input embeddings, attention mask for critic and apply to generate_kwargs\n",
    "        # input_prompts = ['<|system|>:' + sys_prompts['critic'] + '</s>\\n<|question|>:' + question + '</s>\\n<|correct answer|>:'+ answer + '</s>\\n<|student|>:' + actor_response + '</s>\\n<|assistant|>:' for question, answer, actor_response in zip(questions, answers, actor_responses)]\n",
    "        # input_embeds = [critic.get_input_embeddings()(tokenizer(input_prompt, return_tensors='pt', padding=False, truncation=True, max_length=512).input_ids.to(device)) for input_prompt in input_prompts]\n",
    "        # max_len = max([input_embed.size(1) for input_embed in input_embeds])\n",
    "        # attention_mask = torch.concatenate([torch.cat([torch.zeros(max_len - input_embed.size(1), device=device), torch.ones(input_embed.size(1), device=device)]).unsqueeze(0) for input_embed in input_embeds], dim=0).to(dtype=torch.float16)\n",
    "        # input_embeds = torch.concatenate([torch.cat([torch.zeros(1, max_len - input_embed.size(1), embedding_dim, device=device), input_embed], dim=1) for input_embed in input_embeds], dim=0).to(dtype=torch.float16)\n",
    "        # generate_kwargs['inputs_embeds'] = input_embeds\n",
    "        # generate_kwargs['attention_mask'] = attention_mask\n",
    "\n",
    "        # # generate critic responses\n",
    "        # outputs = critic.generate(**generate_kwargs)\n",
    "        # for critic_output in outputs:\n",
    "        #     if tokenizer.eos_token_id in critic_output:\n",
    "        #         eos_idx = (critic_output == tokenizer.eos_token_id).nonzero()[0].item()\n",
    "        #         critic_output[eos_idx+1:] = tokenizer.pad_token_id\n",
    "        # critic_responses = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "        # test if responses include padding tokens\n",
    "        for actor_response in actor_responses:\n",
    "            print(f'actor_response: {actor_response}\\n')\n",
    "        # for critic_response in critic_responses:\n",
    "        #     print(f'critic_response: {critic_response}\\n')\n",
    "            # print(f'last 5 charactor: {actor_response[-5:]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS_RE = re.compile(r\"#### (\\-?[0-9\\.\\,]+)\")\n",
    "def extract_answer(completion):\n",
    "    match = ANS_RE.search(completion)\n",
    "    if match:\n",
    "        match_str = match.group(1).strip()\n",
    "        match_str = match_str.replace(\",\", \"\")\n",
    "        return match_str\n",
    "    else:\n",
    "        return \"[invalid]\"\n",
    "\n",
    "def generate_actor_response(actor, tokenizer, sys_prompt, data, batch_size, device, use_tqdm=False):\n",
    "    n_data = len(data)\n",
    "    n_batches = n_data // batch_size\n",
    "    results = []\n",
    "    stop = BatchedStopOnTokens()\n",
    "    generate_kwargs = dict(\n",
    "        inputs_embeds=None,\n",
    "        max_new_tokens=1024,\n",
    "        do_sample=True,\n",
    "        top_p=0.95,\n",
    "        top_k=50,\n",
    "        temperature=0.7,\n",
    "        num_beams=1,\n",
    "        stopping_criteria=StoppingCriteriaList([stop]),\n",
    "        attention_mask=None\n",
    "    )\n",
    "    if use_tqdm:\n",
    "        pbar = tqdm(total=n_data, desc=f'Data generating for critic response on {device}', ncols=100)\n",
    "    for batch_idx in range(n_batches):\n",
    "        batch_data = data[batch_idx*batch_size:(batch_idx+1)*batch_size]\n",
    "        questions = [d['question'] for d in batch_data]\n",
    "        answers = [d['answer'] for d in batch_data]\n",
    "        ids = [d['id'] for d in batch_data]\n",
    "\n",
    "        # generate batched input embeddings, attention mask for actor and apply to generate_kwargs\n",
    "        input_prompts = ['<|system|>:' + sys_prompt + '</s>\\n<|user|>:' + question + '</s>\\n<|assistant|>:' for question in questions]\n",
    "        input_embeds = [actor.get_input_embeddings()(tokenizer(input_prompt, return_tensors='pt', padding=False, truncation=True, max_length=512).input_ids.to(device)) for input_prompt in input_prompts]\n",
    "        max_len = max([input_embed.size(1) for input_embed in input_embeds])\n",
    "        attention_mask = torch.concatenate([torch.cat([torch.zeros(max_len - input_embed.size(1), device=device), torch.ones(input_embed.size(1), device=device)]).unsqueeze(0) for input_embed in input_embeds], dim=0).to(dtype=torch.float16)\n",
    "        input_embeds = torch.concatenate([torch.cat([torch.zeros(1, max_len - input_embed.size(1), embedding_dim, device=device), input_embed], dim=1) for input_embed in input_embeds], dim=0).to(dtype=torch.float16)\n",
    "        generate_kwargs['inputs_embeds'] = input_embeds\n",
    "        generate_kwargs['attention_mask'] = attention_mask\n",
    "\n",
    "        # generate actor responses\n",
    "        outputs = actor.generate(**generate_kwargs)\n",
    "        for actor_output in outputs:\n",
    "            if tokenizer.eos_token_id in actor_output:\n",
    "                eos_idx = (actor_output == tokenizer.eos_token_id).nonzero()[0].item()\n",
    "                actor_output[eos_idx+1:] = tokenizer.pad_token_id\n",
    "        actor_responses = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "        results.extend(actor_responses)\n",
    "        if use_tqdm:\n",
    "            pbar.update(batch_size)\n",
    "    if use_tqdm:\n",
    "        pbar.close()\n",
    "    return results\n",
    "\n",
    "def generate_critic_response(critic, tokenizer, actor_data, question_data, batch_size, device, use_tqdm=False):\n",
    "    n_data = len(actor_data)\n",
    "    n_batches = n_data // batch_size\n",
    "    results = []\n",
    "    # stop = BatchedStopOnTokens()\n",
    "    generate_kwargs = dict(\n",
    "        inputs_embeds=None,\n",
    "        max_new_tokens=1024,\n",
    "        do_sample=True,\n",
    "        top_p=0.95,\n",
    "        top_k=50,\n",
    "        temperature=0.7,\n",
    "        num_beams=1,\n",
    "        # stopping_criteria=StoppingCriteriaList([stop]),\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=[tokenizer.eos_token_id,tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")],\n",
    "        attention_mask=None\n",
    "    )\n",
    "    if use_tqdm:\n",
    "        pbar = tqdm(total=n_data, desc=f'Data generating end to end on {device}', ncols=100)\n",
    "    for batch_idx in range(n_batches):\n",
    "        batch_data = actor_data[batch_idx*batch_size:(batch_idx+1)*batch_size]\n",
    "        questions = [question_data[d['question_id']]['question'] for d in batch_data]\n",
    "        answers = [extract_answer(question_data[d['question_id']]['answer']) for d in batch_data]\n",
    "        ids = [d['id'] for d in batch_data]\n",
    "        actor_responses = [d['actor_response'] for d in batch_data]\n",
    "\n",
    "        # generate batched input embeddings, attention mask for critic and apply to generate_kwargs\n",
    "        messages = [[{'role': 'system', 'content': 'You are a critic who is responsible for judging the correctness of the actor\\'s answer. Provided with the math problem, correct answer and the student\\'s answer, you need to judge whether the actor\\'s answer is correct. If the actor\\'s answer is right, respond with \"#### The answer is: Accepted.\" Otherwise, analyze the reason why the actor arrived at the wrong answer and respond with \"#### The answer is: Wrong Answer. [Reason for the wrong answer, without displaying the correct number to the question]\".'},\n",
    "                     {'role': 'question', 'content': question},\n",
    "                     {'role': 'correct answer', 'content': answer},\n",
    "                     {'role': 'actor\\'s answer', 'content': actor_response}] for question, answer, actor_response in zip(questions, answers, actor_responses)]\n",
    "        # input_prompts = ['<|system|>:' + sys_prompt + '</s>\\n<|question|>:' + question + '</s>\\n<|correct answer|>:'+ answer + '</s>\\n<|student|>:' + actor_response + '</s>\\n<|assistant|>:' for question, answer, actor_response in zip(questions, answers, actor_responses)]\n",
    "        input_prompts = [tokenizer.apply_chat_template(message, tokenize=False, add_generation_prompt=True) for message in messages]\n",
    "        input_embeds = [critic.get_input_embeddings()(tokenizer(input_prompt, return_tensors='pt', padding=False, truncation=True, max_length=1024).input_ids.to(device)) for input_prompt in input_prompts]\n",
    "        max_len = max([input_embed.size(1) for input_embed in input_embeds])\n",
    "        attention_mask = torch.concatenate([torch.cat([torch.zeros(max_len - input_embed.size(1), device=device), torch.ones(input_embed.size(1), device=device)]).unsqueeze(0) for input_embed in input_embeds], dim=0).to(dtype=torch.float16)\n",
    "        input_embeds = torch.concatenate([torch.cat([torch.zeros(1, max_len - input_embed.size(1), embedding_dim, device=device), input_embed], dim=1) for input_embed in input_embeds], dim=0).to(dtype=torch.float16)\n",
    "        generate_kwargs['inputs_embeds'] = input_embeds\n",
    "        generate_kwargs['attention_mask'] = attention_mask\n",
    "\n",
    "        # generate critic responses\n",
    "        outputs = critic.generate(**generate_kwargs)\n",
    "        for critic_output in outputs:\n",
    "            if tokenizer.eos_token_id in critic_output:\n",
    "                eos_idx = (critic_output == tokenizer.eos_token_id).nonzero()[0].item()\n",
    "                critic_output[eos_idx+1:] = tokenizer.pad_token_id\n",
    "        critic_responses = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "        results.extend(critic_responses)\n",
    "        if use_tqdm:\n",
    "            pbar.update(batch_size)\n",
    "    if use_tqdm:\n",
    "        pbar.close()\n",
    "    return results\n",
    "\n",
    "def generate_summarizer_response(critic, tokenizer, critic_data, actor_data, question_data, batch_size, device, use_tqdm=False):\n",
    "    n_data = len(critic_data)\n",
    "    n_batches = n_data // batch_size\n",
    "    results = []\n",
    "    # stop = BatchedStopOnTokens()\n",
    "    generate_kwargs = dict(\n",
    "        inputs_embeds=None,\n",
    "        max_new_tokens=1024,\n",
    "        do_sample=True,\n",
    "        top_p=0.95,\n",
    "        top_k=50,\n",
    "        temperature=0.7,\n",
    "        num_beams=1,\n",
    "        # stopping_criteria=StoppingCriteriaList([stop]),\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=[tokenizer.eos_token_id,tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")],\n",
    "        attention_mask=None\n",
    "    )\n",
    "    if use_tqdm:\n",
    "        pbar = tqdm(total=n_data, desc=f'Generating summarizer response data on {device}', ncols=100)\n",
    "    for batch_idx in range(n_batches):\n",
    "        batch_data = critic_data[batch_idx*batch_size:(batch_idx+1)*batch_size]\n",
    "        actor_data_dicts = [actor_data[d['actor_response_id']] for d in batch_data]\n",
    "        question_data_dicts = [question_data[d['question_id']] for d in actor_data_dicts]\n",
    "        questions = [d['question'] for d in question_data_dicts] \n",
    "        answers = [extract_answer(d['answer']) for d in question_data_dicts]\n",
    "        ids = [d['id'] for d in batch_data]\n",
    "        actor_responses = [d['actor_response'] for d in actor_data_dicts]\n",
    "        critic_responses = [d['critic_response'] for d in batch_data]\n",
    "\n",
    "        # generate batched input embeddings, attention mask for critic and apply to generate_kwargs\n",
    "        messages = [[{'role': 'system', 'content': 'You are a summarizer who is responsible for deciding the final answer to a given math problem, with the help of an actor\\'s solution and a critic\\'s judgement of whether the actor\\'s answer is correct or not. If the actor\\'s answer is correct, summarize the analysis. Otherwise, fix the actor\\'s answer according to the critic\\'s feedback. Only the correct analysis is allowed to be presented. Do not include statements about whether the actor or critic is correct. Finally, add \"\\n\\n#### [Answer to the question with digits only]\" as a summarization.'},\n",
    "                     {'role': 'question', 'content': question},\n",
    "                     {'role': 'actor\\'s answer', 'content': actor_response},\n",
    "                     {'role': 'critic\\'s judgement', 'contenet': critic_response}] for question, actor_response, critic_response in zip(questions, actor_responses, critic_responses)]\n",
    "        # input_prompts = ['<|system|>:' + sys_prompt + '</s>\\n<|question|>:' + question + '</s>\\n<|correct answer|>:'+ answer + '</s>\\n<|student|>:' + actor_response + '</s>\\n<|assistant|>:' for question, answer, actor_response in zip(questions, answers, actor_responses)]\n",
    "        input_prompts = [tokenizer.apply_chat_template(message, tokenize=False, add_generation_prompt=True) for message in messages]\n",
    "        input_embeds = [critic.get_input_embeddings()(tokenizer(input_prompt, return_tensors='pt', padding=False, truncation=True, max_length=1024).input_ids.to(device)) for input_prompt in input_prompts]\n",
    "        max_len = max([input_embed.size(1) for input_embed in input_embeds])\n",
    "        attention_mask = torch.concatenate([torch.cat([torch.zeros(max_len - input_embed.size(1), device=device), torch.ones(input_embed.size(1), device=device)]).unsqueeze(0) for input_embed in input_embeds], dim=0).to(dtype=torch.float16)\n",
    "        input_embeds = torch.concatenate([torch.cat([torch.zeros(1, max_len - input_embed.size(1), embedding_dim, device=device), input_embed], dim=1) for input_embed in input_embeds], dim=0).to(dtype=torch.float16)\n",
    "        generate_kwargs['inputs_embeds'] = input_embeds\n",
    "        generate_kwargs['attention_mask'] = attention_mask\n",
    "\n",
    "        # generate critic responses\n",
    "        outputs = critic.generate(**generate_kwargs)\n",
    "        for critic_output in outputs:\n",
    "            if tokenizer.eos_token_id in critic_output:\n",
    "                eos_idx = (critic_output == tokenizer.eos_token_id).nonzero()[0].item()\n",
    "                critic_output[eos_idx+1:] = tokenizer.pad_token_id\n",
    "        critic_responses = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "        results.extend(critic_responses)\n",
    "        if use_tqdm:\n",
    "            pbar.update(batch_size)\n",
    "    if use_tqdm:\n",
    "        pbar.close()\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_generate_e2e(models[0], models[0], summarizer_models[0], tokenizers[0], {'actor': \"Solving the following math problem and respond with '\\n#### <answer>' with <answer> substituted by the correct number in the very end:\\n\", 'critic': \"Judge the student's answer (not solving the problem by yourself) to the following math question with the given correct answer. Respond with '\\n#### accepted' if the student is correct, or '\\n#### wrong answer: <why the answer is wrong>' if the student's answer is wrong.\\n\"}, data_use[4:6], 2, torch.device('cuda:0'), use_tqdm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Data generating end to end on cuda:0: 100%|███████████████████████████| 4/4 [00:01<00:00,  2.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### The answer is: Wrong Answer. Jackson's calculation for the total number of calories in the salad and pizza is correct, but then he adds the number of calories in the salad and pizza to get 132.5, which is incorrect.\n",
      "Wrong Answer\n",
      "#### The answer is: Accepted.\n",
      "Accepted\n",
      "#### The answer is: Accepted.\n",
      "Accepted\n",
      "#### The answer is: Accepted.\n",
      "Accepted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# actor_responses = generate_actor_response(actor_models[0], tokenizers[0], \"Solving the following math problem and respond with '\\n#### <answer>' with <answer> substituted by the correct number in the very end:\\n\", [data_use[0]] * 4, 2, torch.device('cuda:0'), use_tqdm=True)\n",
    "# for actor_response in actor_responses:\n",
    "#     print(actor_response)\n",
    "# critic_responses = generate_critic_response(critic_models[0], tokenizers[4], \"Judge the student's answer (not solving the problem by yourself) to the following math question using the <correct answer> provided as reference. If the student is correct, respond with '\\n#### accepted.' Otherwise, analyse why the student is wrong and respond with '\\n#### wrong answer: <why the student is wrong>'.\\n\", data_use[0:2], actor_responses, 2, torch.device('cuda:4'), use_tqdm=True)\n",
    "# print(critic_responses)\n",
    "critic_responses = generate_critic_response(critic_models[0], tokenizers[0], actor_response_data[:4], data_train, 4, torch.device('cuda:0'), use_tqdm=True) \n",
    "for critic_response in critic_responses:\n",
    "    print(critic_response)\n",
    "# summarizer_responses = generate_summarizer_response(summarizer_models[0], tokenizers[0], critic_response_data[:4], actor_response_data, data_train, 4, torch.device('cuda:0'), use_tqdm=True)\n",
    "# for summarizer_response in summarizer_responses:\n",
    "#     print(summarizer_response)\n",
    "# print(len(actor_responses), len(critic_responses), len(summarizer_responses))\n",
    "    print(extract_judgement(critic_response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_judgement(output):\n",
    "    if output.startswith(\"#### The answer is: Accepted.\"):\n",
    "        return 'Accepted'\n",
    "    if output.startswith(\"#### The answer is: Wrong Answer.\"):\n",
    "        return 'Wrong Answer'\n",
    "    return 'Invalid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wrong Answer', 'Accepted', 'Accepted', 'Accepted']\n"
     ]
    }
   ],
   "source": [
    "print([extract_judgement(critic_response) for critic_response in critic_responses])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gradio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
