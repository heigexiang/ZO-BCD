{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList, TextIteratorStreamer\n",
    "from threading import Thread\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0,1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "/mnt/xue.w/models/hub/models--miqudev--miqu-1-70b/models--miqudev--miqu-1-70b/snapshots/82f0daa6767263aa5990dea54dbb13e94d096de7 does not appear to have a file named config.json. Checkout 'https://huggingface.co//mnt/xue.w/models/hub/models--miqudev--miqu-1-70b/models--miqudev--miqu-1-70b/snapshots/82f0daa6767263aa5990dea54dbb13e94d096de7/None' for available files.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 16\u001b[0m\n\u001b[1;32m     10\u001b[0m model_pth \u001b[38;5;241m=\u001b[39m miqu_70b\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Loading the tokenizer and model from Hugging Face's model hub.\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_pth\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mload_in_8bit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_pth,torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16,device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,load_in_4bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,load_in_8bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# using CUDA for an optimal experience\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/gradio/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:773\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config_tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[0;32m--> 773\u001b[0m         config \u001b[38;5;241m=\u001b[39m \u001b[43mAutoConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    774\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    775\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    776\u001b[0m     config_tokenizer_class \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mtokenizer_class\n\u001b[1;32m    777\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mauto_map:\n",
      "File \u001b[0;32m~/anaconda3/envs/gradio/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:1100\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1097\u001b[0m trust_remote_code \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrust_remote_code\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   1098\u001b[0m code_revision \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode_revision\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m-> 1100\u001b[0m config_dict, unused_kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mPretrainedConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1101\u001b[0m has_remote_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1102\u001b[0m has_local_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m CONFIG_MAPPING\n",
      "File \u001b[0;32m~/anaconda3/envs/gradio/lib/python3.10/site-packages/transformers/configuration_utils.py:634\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    632\u001b[0m original_kwargs \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(kwargs)\n\u001b[1;32m    633\u001b[0m \u001b[38;5;66;03m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m config_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict:\n\u001b[1;32m    636\u001b[0m     original_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/gradio/lib/python3.10/site-packages/transformers/configuration_utils.py:689\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    685\u001b[0m configuration_file \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_configuration_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, CONFIG_NAME)\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    688\u001b[0m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[0;32m--> 689\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfiguration_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    703\u001b[0m     commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;66;03m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# the original exception.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/gradio/lib/python3.10/site-packages/transformers/utils/hub.py:356\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(resolved_file):\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _raise_exceptions_for_missing_entries:\n\u001b[0;32m--> 356\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    357\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not appear to have a file named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfull_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Checkout \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    358\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for available files.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    359\u001b[0m         )\n\u001b[1;32m    360\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    361\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mOSError\u001b[0m: /mnt/xue.w/models/hub/models--miqudev--miqu-1-70b/models--miqudev--miqu-1-70b/snapshots/82f0daa6767263aa5990dea54dbb13e94d096de7 does not appear to have a file named config.json. Checkout 'https://huggingface.co//mnt/xue.w/models/hub/models--miqudev--miqu-1-70b/models--miqudev--miqu-1-70b/snapshots/82f0daa6767263aa5990dea54dbb13e94d096de7/None' for available files."
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/blog/llama2#using-transformers\n",
    "\n",
    "\n",
    "tiny_llama = \"/mnt/xue.w/models/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/77e23968eed12d195bd46c519aa679cc22a27ddc\"\n",
    "llama_7b_hf_chat = \"/mnt/xue.w/models/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/c1b0db933684edbfe29a06fa47eb19cc48025e93\"\n",
    "# code_llama_70 = '/mnt/xue.w/models/hub/models--codellama--CodeLlama-70b-hf/snapshots/4570a4edc524fb9f20f605b417bb43828fa5997a'\n",
    "\n",
    "miqu_70b ='/mnt/xue.w/models/hub/models--miqudev--miqu-1-70b/models--miqudev--miqu-1-70b/snapshots/82f0daa6767263aa5990dea54dbb13e94d096de7'\n",
    "Mixtral_8x7b_instruct ='/mnt/xue.w/models/hub/models--mistralai--Mixtral-8x7B-Instruct-v0.1/snapshots/125c431e2ff41a156b9f9076f744d2f35dd6e67a'\n",
    "model_pth = Mixtral_8x7b_instruct\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Loading the tokenizer and model from Hugging Face's model hub.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_pth,torch_dtype=torch.float16,device_map=\"auto\",load_in_4bit=False,load_in_8bit=False)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_pth,torch_dtype=torch.float16,device_map=\"auto\",load_in_4bit=False,load_in_8bit=False)\n",
    "\n",
    "# using CUDA for an optimal experience\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        stop_ids = [2]  # IDs of tokens where the generation should stop.\n",
    "        for stop_id in stop_ids:\n",
    "            if input_ids[0][-1] == stop_id:  # Checking if the last generated token is a stop token.\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "\n",
    "# Function to generate model predictions.\n",
    "def predict(message,history = []):\n",
    "    history_transformer_format = history + [[message, \"\"]]\n",
    "    stop = StopOnTokens()\n",
    "\n",
    "    # Formatting the input for the model.\n",
    "    messages = \"</s>\".join([\"</s>\".join([\"\\n<|user|>:\" + item[0], \"\\n<|assistant|>:\" + item[1]])\n",
    "                        for item in history_transformer_format])\n",
    "    model_inputs = tokenizer([messages], return_tensors=\"pt\")\n",
    "    # .to(device)\n",
    "    streamer = TextIteratorStreamer(tokenizer, timeout=10., skip_prompt=True, skip_special_tokens=True)\n",
    "    generate_kwargs = dict(\n",
    "        model_inputs,\n",
    "        streamer=streamer,\n",
    "        max_new_tokens=1024,\n",
    "        do_sample=True,\n",
    "        top_p=0.95,\n",
    "        top_k=50,\n",
    "        temperature=0.7,\n",
    "        num_beams=1,\n",
    "        stopping_criteria=StoppingCriteriaList([stop])\n",
    "    )\n",
    "    t = Thread(target=model.generate, kwargs=generate_kwargs)\n",
    "    t.start()  # Starting the generation in a separate thread.\n",
    "    partial_message = \"\"\n",
    "    for new_token in streamer:\n",
    "        partial_message += new_token\n",
    "        if '</s>' in partial_message:  # Breaking the loop if the stop token is generated.\n",
    "            break\n",
    "    yield partial_message\n",
    "    # return partial_message\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"How old are you?\" is a question that asks about a person's age, while \"How are you?\" is a question that asks about a person's current state of being, often referring to their health or mood.\n"
     ]
    }
   ],
   "source": [
    "a0 = 'What is the difference between How old are you and how are you?'\n",
    "for i in predict(a0):\n",
    "    print(i)\n",
    "# a0 = 'What is the difference between How old are you and how are you?'\n",
    "# a = predict(a0)\n",
    "# print(a)\n",
    "# print('---'*20)\n",
    "# a0 = 'If 1 = 6, 2 = 5, 3=4, what should 5 = ?'\n",
    "# a = predict(a0)\n",
    "# print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Given \n",
      "Given the \n",
      "Given the \n",
      "Given the pattern, \n",
      "Given the pattern, if \n",
      "Given the pattern, if \n",
      "Given the pattern, if 1 \n",
      "Given the pattern, if 1 = \n",
      "Given the pattern, if 1 = \n",
      "Given the pattern, if 1 = \n",
      "Given the pattern, if 1 = 6, \n",
      "Given the pattern, if 1 = 6, \n",
      "Given the pattern, if 1 = 6, 2 \n",
      "Given the pattern, if 1 = 6, 2 = \n",
      "Given the pattern, if 1 = 6, 2 = \n",
      "Given the pattern, if 1 = 6, 2 = \n",
      "Given the pattern, if 1 = 6, 2 = 5, \n",
      "Given the pattern, if 1 = 6, 2 = 5, and \n",
      "Given the pattern, if 1 = 6, 2 = 5, and \n",
      "Given the pattern, if 1 = 6, 2 = 5, and 3 \n",
      "Given the pattern, if 1 = 6, 2 = 5, and 3 = \n",
      "Given the pattern, if 1 = 6, 2 = 5, and 3 = \n",
      "Given the pattern, if 1 = 6, 2 = 5, and 3 = \n",
      "Given the pattern, if 1 = 6, 2 = 5, and 3 = 4, \n",
      "Given the pattern, if 1 = 6, 2 = 5, and 3 = 4, then \n",
      "Given the pattern, if 1 = 6, 2 = 5, and 3 = 4, then \n",
      "Given the pattern, if 1 = 6, 2 = 5, and 3 = 4, then 5 \n",
      "Given the pattern, if 1 = 6, 2 = 5, and 3 = 4, then 5 would \n",
      "Given the pattern, if 1 = 6, 2 = 5, and 3 = 4, then 5 would equal \n",
      "Given the pattern, if 1 = 6, 2 = 5, and 3 = 4, then 5 would equal \n",
      "Given the pattern, if 1 = 6, 2 = 5, and 3 = 4, then 5 would equal \n",
      "Given the pattern, if 1 = 6, 2 = 5, and 3 = 4, then 5 would equal 2. \n",
      "Given the pattern, if 1 = 6, 2 = 5, and 3 = 4, then 5 would equal 2. This \n",
      "Given the pattern, if 1 = 6, 2 = 5, and 3 = 4, then 5 would equal 2. This is \n",
      "Given the pattern, if 1 = 6, 2 = 5, and 3 = 4, then 5 would equal 2. This is because \n",
      "Given the pattern, if 1 = 6, 2 = 5, and 3 = 4, then 5 would equal 2. This is because the \n",
      "Given the pattern, if 1 = 6, 2 = 5, and 3 = 4, then 5 would equal 2. This is because the numbers \n",
      "Given the pattern, if 1 = 6, 2 = 5, and 3 = 4, then 5 would equal 2. This is because the numbers are \n",
      "Given the pattern, if 1 = 6, 2 = 5, and 3 = 4, then 5 would equal 2. This is because the numbers are \n",
      "Given the pattern, if 1 = 6, 2 = 5, and 3 = 4, then 5 would equal 2. This is because the numbers are decreasing \n",
      "Given the pattern, if 1 = 6, 2 = 5, and 3 = 4, then 5 would equal 2. This is because the numbers are decreasing by \n",
      "Given the pattern, if 1 = 6, 2 = 5, and 3 = 4, then 5 would equal 2. This is because the numbers are decreasing by \n",
      "Given the pattern, if 1 = 6, 2 = 5, and 3 = 4, then 5 would equal 2. This is because the numbers are decreasing by 1 \n",
      "Given the pattern, if 1 = 6, 2 = 5, and 3 = 4, then 5 would equal 2. This is because the numbers are decreasing by 1 as \n",
      "Given the pattern, if 1 = 6, 2 = 5, and 3 = 4, then 5 would equal 2. This is because the numbers are decreasing by 1 as they \n",
      "Given the pattern, if 1 = 6, 2 = 5, and 3 = 4, then 5 would equal 2. This is because the numbers are decreasing by 1 as they increase \n",
      "Given the pattern, if 1 = 6, 2 = 5, and 3 = 4, then 5 would equal 2. This is because the numbers are decreasing by 1 as they increase by \n",
      "Given the pattern, if 1 = 6, 2 = 5, and 3 = 4, then 5 would equal 2. This is because the numbers are decreasing by 1 as they increase by \n",
      "Given the pattern, if 1 = 6, 2 = 5, and 3 = 4, then 5 would equal 2. This is because the numbers are decreasing by 1 as they increase by \n",
      "Given the pattern, if 1 = 6, 2 = 5, and 3 = 4, then 5 would equal 2. This is because the numbers are decreasing by 1 as they increase by 1. \n",
      "Given the pattern, if 1 = 6, 2 = 5, and 3 = 4, then 5 would equal 2. This is because the numbers are decreasing by 1 as they increase by 1. \n",
      "Given the pattern, if 1 = 6, 2 = 5, and 3 = 4, then 5 would equal 2. This is because the numbers are decreasing by 1 as they increase by 1. So, \n",
      "Given the pattern, if 1 = 6, 2 = 5, and 3 = 4, then 5 would equal 2. This is because the numbers are decreasing by 1 as they increase by 1. So, \n",
      "Given the pattern, if 1 = 6, 2 = 5, and 3 = 4, then 5 would equal 2. This is because the numbers are decreasing by 1 as they increase by 1. So, 5 \n",
      "Given the pattern, if 1 = 6, 2 = 5, and 3 = 4, then 5 would equal 2. This is because the numbers are decreasing by 1 as they increase by 1. So, 5 would \n",
      "Given the pattern, if 1 = 6, 2 = 5, and 3 = 4, then 5 would equal 2. This is because the numbers are decreasing by 1 as they increase by 1. So, 5 would be \n",
      "Given the pattern, if 1 = 6, 2 = 5, and 3 = 4, then 5 would equal 2. This is because the numbers are decreasing by 1 as they increase by 1. So, 5 would be the \n",
      "Given the pattern, if 1 = 6, 2 = 5, and 3 = 4, then 5 would equal 2. This is because the numbers are decreasing by 1 as they increase by 1. So, 5 would be the new \n",
      "Given the pattern, if 1 = 6, 2 = 5, and 3 = 4, then 5 would equal 2. This is because the numbers are decreasing by 1 as they increase by 1. So, 5 would be the new \n",
      "Given the pattern, if 1 = 6, 2 = 5, and 3 = 4, then 5 would equal 2. This is because the numbers are decreasing by 1 as they increase by 1. So, 5 would be the new \n",
      "Given the pattern, if 1 = 6, 2 = 5, and 3 = 4, then 5 would equal 2. This is because the numbers are decreasing by 1 as they increase by 1. So, 5 would be the new 6, \n",
      "Given the pattern, if 1 = 6, 2 = 5, and 3 = 4, then 5 would equal 2. This is because the numbers are decreasing by 1 as they increase by 1. So, 5 would be the new 6, and \n",
      "Given the pattern, if 1 = 6, 2 = 5, and 3 = 4, then 5 would equal 2. This is because the numbers are decreasing by 1 as they increase by 1. So, 5 would be the new 6, and \n",
      "Given the pattern, if 1 = 6, 2 = 5, and 3 = 4, then 5 would equal 2. This is because the numbers are decreasing by 1 as they increase by 1. So, 5 would be the new 6, and 6 \n",
      "Given the pattern, if 1 = 6, 2 = 5, and 3 = 4, then 5 would equal 2. This is because the numbers are decreasing by 1 as they increase by 1. So, 5 would be the new 6, and 6 is \n",
      "Given the pattern, if 1 = 6, 2 = 5, and 3 = 4, then 5 would equal 2. This is because the numbers are decreasing by 1 as they increase by 1. So, 5 would be the new 6, and 6 is \n",
      "Given the pattern, if 1 = 6, 2 = 5, and 3 = 4, then 5 would equal 2. This is because the numbers are decreasing by 1 as they increase by 1. So, 5 would be the new 6, and 6 is 2 \n",
      "Given the pattern, if 1 = 6, 2 = 5, and 3 = 4, then 5 would equal 2. This is because the numbers are decreasing by 1 as they increase by 1. So, 5 would be the new 6, and 6 is 2 in \n",
      "Given the pattern, if 1 = 6, 2 = 5, and 3 = 4, then 5 would equal 2. This is because the numbers are decreasing by 1 as they increase by 1. So, 5 would be the new 6, and 6 is 2 in this \n",
      "Given the pattern, if 1 = 6, 2 = 5, and 3 = 4, then 5 would equal 2. This is because the numbers are decreasing by 1 as they increase by 1. So, 5 would be the new 6, and 6 is 2 in this \n",
      "Given the pattern, if 1 = 6, 2 = 5, and 3 = 4, then 5 would equal 2. This is because the numbers are decreasing by 1 as they increase by 1. So, 5 would be the new 6, and 6 is 2 in this \n",
      "Given the pattern, if 1 = 6, 2 = 5, and 3 = 4, then 5 would equal 2. This is because the numbers are decreasing by 1 as they increase by 1. So, 5 would be the new 6, and 6 is 2 in this pattern.\n"
     ]
    }
   ],
   "source": [
    "a0 = 'If 1 = 6, 2 = 5, 3=4, what should 5 = ?'\n",
    "for i in predict(a0):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gradio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
