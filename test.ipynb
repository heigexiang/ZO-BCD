{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import argparse\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from peft import (\n",
    "        get_peft_model, \n",
    "        prepare_model_for_kbit_training, \n",
    "        LoraConfig\n",
    "    )\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.19s/it]\n"
     ]
    }
   ],
   "source": [
    "model_path = model_path = \"/mnt/xue.w/models/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/c1b0db933684edbfe29a06fa47eb19cc48025e93\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path,\n",
    "                                            load_in_8bit=True,\n",
    "                                            device_map=\"auto\"\n",
    "                                            )\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 14732 examples [00:00, 101996.48 examples/s]\n",
      "Generating validation split: 818 examples [00:00, 116027.75 examples/s]\n",
      "Generating test split: 819 examples [00:00, 132288.48 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['summary', 'dialogue', 'id'],\n",
      "    num_rows: 14732\n",
      "}) Dataset({\n",
      "    features: ['summary', 'dialogue', 'id'],\n",
      "    num_rows: 819\n",
      "}) Dataset({\n",
      "    features: ['summary', 'dialogue', 'id'],\n",
      "    num_rows: 818\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'summary': 'Amanda baked cookies and will bring Jerry some tomorrow.',\n",
       " 'dialogue': \"Amanda: I baked  cookies. Do you want some?\\r\\nJerry: Sure!\\r\\nAmanda: I'll bring you tomorrow :-)\",\n",
       " 'id': '13818513'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "data = load_dataset(\"/mnt/yutong/data/samsum\")\n",
    "data_train, data_test, data_val = data[\"train\"], data[\"test\"], data[\"validation\"]\n",
    "\n",
    "print(data_train, data_test, data_val)\n",
    "\n",
    "# example\n",
    "data_train[0]\n",
    "# output\n",
    "#{'id': '13818513',\n",
    "# 'dialogue': \"Amanda: I baked  cookies. Do you want some?\\r\\nJerry: Sure!\\r\\nAmanda: I'll bring you tomorrow :-)\",\n",
    "# 'summary': 'Amanda baked cookies and will bring Jerry some tomorrow.'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarize the following:\n",
      " Amanda: I baked  cookies. Do you want some?\n",
      "Jerry: Sure!\n",
      "Amanda: I'll bring you tomorrow :-)\n",
      " Summary: Amanda baked cookies and will bring Jerry some tomorrow. </s> \n"
     ]
    }
   ],
   "source": [
    "def generate_prompt(dialogue, summary=None, eos_token=\"</s>\"):\n",
    "  instruction = \"Summarize the following:\\n\"\n",
    "  input = f\"{dialogue}\\n\"\n",
    "  summary = f\"Summary: {summary + ' ' + eos_token if summary else ''} \"\n",
    "  prompt = (\" \").join([instruction, input, summary])\n",
    "  return prompt\n",
    "\n",
    "print(generate_prompt(data_train[0][\"dialogue\"], data_train[0][\"summary\"]))\n",
    "# Summarize the following:\n",
    "# Amanda: I baked  cookies. Do you want some?\n",
    "# Jerry: Sure!\n",
    "# Amanda: I'll bring you tomorrow :-)\n",
    "#  Summary: Amanda baked cookies and will bring Jerry some tomorrow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarize the following:\n",
      " Pitt: Hey Teddy! Have you received my message?\n",
      "Teddy: No. An email?\n",
      "Pitt: No. On the FB messenger.\n",
      "Teddy: Let me check.\n",
      "Teddy: Yeah. Ta!\n",
      " Summary:   Pitt sent a message to Teddy on Facebook Messenger, but Teddy did not receive it. Teddy checked and found that he had indeed received the message.\n"
     ]
    }
   ],
   "source": [
    "input_prompt = generate_prompt(data_train[50][\"dialogue\"])\n",
    "input_tokens = tokenizer(input_prompt, return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\")\n",
    "with torch.cuda.amp.autocast():\n",
    "  generation_output = model.generate(\n",
    "      input_ids=input_tokens,\n",
    "      max_new_tokens=1000,\n",
    "      do_sample=True,\n",
    "      top_k=10,\n",
    "      top_p=0.9,\n",
    "      temperature=0.3,\n",
    "      repetition_penalty=1.15,\n",
    "      num_return_sequences=1,\n",
    "      eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "op = tokenizer.decode(generation_output[0], skip_special_tokens=True)\n",
    "print(op)\n",
    "# Summarize the following:\n",
    "#  Pitt: Hey Teddy! Have you received my message?\n",
    "# Teddy: No. An email?\n",
    "# Pitt: No. On the FB messenger.\n",
    "# Teddy: Let me check.\n",
    "# Teddy: Yeah. Ta!\n",
    "# Summary: 1) What is the purpose of this conversation? 2) Who initiates it and who responds to it? 3) How do they communicate with each other? 4) What does the speaker think about his friend’s response?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=8,\n",
    "        lora_dropout=0.1,\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"],\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaSdpaAttention(\n",
      "          (q_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear8bitLt(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "# LlamaForCausalLM(\n",
    "#  (model): LlamaModel(\n",
    "#     (embed_tokens): Embedding(32000, 4096)\n",
    "#     (layers): ModuleList(\n",
    "#      (0-31): 32 x LlamaDecoderLayer(\n",
    "#        (self_attn): LlamaAttention(\n",
    "#          (q_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
    "#          (k_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
    "#          (v_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
    "#          (o_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
    "#          (rotary_emb): LlamaRotaryEmbedding()\n",
    "#        )\n",
    "#        (mlp): LlamaMLP(\n",
    "#          (gate_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
    "#          (up_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
    "#          (down_proj): Linear8bitLt(in_features=11008, out_features=4096, bias=False)\n",
    "#          (act_fn): SiLUActivation()\n",
    "#        )\n",
    "#        (input_layernorm): LlamaRMSNorm()\n",
    "#        (post_attention_layernorm): LlamaRMSNorm()\n",
    "#      )\n",
    "#    )\n",
    "#    (norm): LlamaRMSNorm()\n",
    "#  )\n",
    "#  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(32001, 4096)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this should be set for finutning and batched inference\n",
    "tokenizer.add_special_tokens({\"pad_token\": \"<PAD>\"})\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading in 8 bit ...\"\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"cp\"\n",
    "per_device_train_batch_size = 4\n",
    "gradient_accumulation_steps = 4\n",
    "per_device_eval_batch_size = 4\n",
    "eval_accumulation_steps = 4\n",
    "optim = \"paged_adamw_32bit\"\n",
    "save_steps = 10\n",
    "logging_steps = 10\n",
    "learning_rate = 5e-4\n",
    "max_grad_norm = 0.3\n",
    "max_steps = 50\n",
    "warmup_ratio = 0.03\n",
    "evaluation_strategy=\"steps\"\n",
    "lr_scheduler_type = \"constant\"\n",
    "\n",
    "training_args = transformers.TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            per_device_train_batch_size=per_device_train_batch_size,\n",
    "            gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "            optim=optim,\n",
    "            evaluation_strategy=evaluation_strategy,\n",
    "            save_steps=save_steps,\n",
    "            learning_rate=learning_rate,\n",
    "            logging_steps=logging_steps,\n",
    "            max_grad_norm=max_grad_norm,\n",
    "            max_steps=max_steps,\n",
    "            warmup_ratio=warmup_ratio,\n",
    "            group_by_length=True,\n",
    "            lr_scheduler_type=lr_scheduler_type,\n",
    "            ddp_find_unused_parameters=False,\n",
    "            eval_accumulation_steps=eval_accumulation_steps,\n",
    "            per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 14732/14732 [00:00<00:00, 16357.63 examples/s]\n",
      "Map: 100%|██████████| 818/818 [00:00<00:00, 17324.92 examples/s]\n",
      "/root/anaconda3/envs/gradio-peft/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n",
      "Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/root/anaconda3/envs/gradio-peft/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/gradio-peft/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 10:13, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.204500</td>\n",
       "      <td>2.050266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.891200</td>\n",
       "      <td>1.819157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.760700</td>\n",
       "      <td>1.780261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.648400</td>\n",
       "      <td>1.779862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.546100</td>\n",
       "      <td>1.946659</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/gradio-peft/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/xue.w/models/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/c1b0db933684edbfe29a06fa47eb19cc48025e93 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/gradio-peft/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/gradio-peft/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/root/anaconda3/envs/gradio-peft/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/xue.w/models/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/c1b0db933684edbfe29a06fa47eb19cc48025e93 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/gradio-peft/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/gradio-peft/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/root/anaconda3/envs/gradio-peft/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/xue.w/models/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/c1b0db933684edbfe29a06fa47eb19cc48025e93 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/gradio-peft/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/gradio-peft/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/root/anaconda3/envs/gradio-peft/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/xue.w/models/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/c1b0db933684edbfe29a06fa47eb19cc48025e93 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/gradio-peft/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/gradio-peft/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/root/anaconda3/envs/gradio-peft/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/xue.w/models/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/c1b0db933684edbfe29a06fa47eb19cc48025e93 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/gradio-peft/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/xue.w/models/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/c1b0db933684edbfe29a06fa47eb19cc48025e93 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def formatting_func(prompt):\n",
    "  output = []\n",
    "\n",
    "  for d, s in zip(prompt[\"dialogue\"], prompt[\"summary\"]):\n",
    "    op = generate_prompt(d, s)\n",
    "    output.append(op)\n",
    "\n",
    "  return output\n",
    "\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=data_train,\n",
    "    eval_dataset=data_val,\n",
    "    peft_config=lora_config,\n",
    "    formatting_func=formatting_func,\n",
    "    max_seq_length=1024,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args\n",
    ")\n",
    "\n",
    "# We will also pre-process the model by upcasting the layer norms in float 32 for more stable training\n",
    "for name, module in trainer.model.named_modules():\n",
    "    if \"norm\" in name:\n",
    "        module = module.to(torch.float32)\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(f\"{output_dir}/final\")\n",
    "\n",
    "# Step Training Loss Validation Loss\n",
    "# 10 1.848200 1.746341\n",
    "# 20 1.688300 1.696681\n",
    "# 30 1.654500 1.698127\n",
    "# 40 1.579400 1.652010\n",
    "# 50 1.492600 1.701877"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "peft_model_id = \"cp/checkpoint-40\"\n",
    "peft_model = PeftModel.from_pretrained(model, peft_model_id, torch_dtype=torch.float16, offload_folder=\"lora_results/lora_7/temp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/gradio-peft/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarize the following:\n",
      " Pitt: Hey Teddy! Have you received my message?\n",
      "Teddy: No. An email?\n",
      "Pitt: No. On the FB messenger.\n",
      "Teddy: Let me check.\n",
      "Teddy: Yeah. Ta!\n",
      " Summary:   Pitt sent a message to Teddy on Facebook Messenger, but Teddy did not receive it. Teddy checks his messages and finds the one from Pitt.\n"
     ]
    }
   ],
   "source": [
    "input_prompt = generate_prompt(data_train[50][\"dialogue\"])\n",
    "input_tokens = tokenizer(input_prompt, return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\")\n",
    "with torch.cuda.amp.autocast():\n",
    "    generation_output = peft_model.generate(\n",
    "        input_ids=input_tokens,\n",
    "        max_new_tokens=100,\n",
    "        do_sample=True,\n",
    "        top_k=10,\n",
    "        top_p=0.9,\n",
    "        temperature=0.3,\n",
    "        repetition_penalty=1.15,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "      )\n",
    "op = tokenizer.decode(generation_output[0], skip_special_tokens=True)\n",
    "print(op)\n",
    "# Summarize the following:\n",
    "#  Pitt: Hey Teddy! Have you received my message?\n",
    "# Teddy: No. An email?\n",
    "# Pitt: No. On the FB messenger.\n",
    "# Teddy: Let me check.\n",
    "# Teddy: Yeah. Ta!\n",
    "#  Summary:   Pitt sent a message to Teddy on Facebook Messenger, but he didn't receive it yet. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gradio-peft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
