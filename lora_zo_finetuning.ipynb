{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 零阶法训练lora中关键代码的编写测试"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 适合Trainer计算loss的数据集构造（参考toolbench的training代码）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 探究LLaMA3 tokenizer所需offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList, TextIteratorStreamer\n",
    "from threading import Thread\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0,1,2,3,4,5,6,7'\n",
    "import json, pickle, copy\n",
    "from peft import PeftModel\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import concurrent.futures\n",
    "from transformers.trainer_pt_utils import LabelSmoother"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.53it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load LLaMA3 model\n",
    "model_pth = '../Meta-Llama-3-8B-Instruct'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_pth,torch_dtype=torch.float16,load_in_4bit=False,load_in_8bit=False)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_pth,torch_dtype=torch.float16,load_in_4bit=False,load_in_8bit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.bos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are GPT5.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is the meaning of life?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "token length: 28\n",
      "token_ids: [128000, 128006, 9125, 128007, 271, 2675, 527, 480, 2898, 20, 13, 128009, 128006, 882, 128007, 271, 3923, 374, 279, 7438, 315, 2324, 30, 128009, 128006, 78191, 128007, 271]\n",
      "<|begin_of_text|>(128000)\n",
      "<|start_header_id|>(128006)\n",
      "system(9125)\n",
      "<|end_header_id|>(128007)\n",
      "\n",
      "\n",
      "(271)\n",
      "You(2675)\n",
      " are(527)\n",
      " G(480)\n",
      "PT(2898)\n",
      "5(20)\n",
      ".(13)\n",
      "<|eot_id|>(128009)\n",
      "<|start_header_id|>(128006)\n",
      "user(882)\n",
      "<|end_header_id|>(128007)\n",
      "\n",
      "\n",
      "(271)\n",
      "What(3923)\n",
      " is(374)\n",
      " the(279)\n",
      " meaning(7438)\n",
      " of(315)\n",
      " life(2324)\n",
      "?(30)\n",
      "<|eot_id|>(128009)\n",
      "<|start_header_id|>(128006)\n",
      "assistant(78191)\n",
      "<|end_header_id|>(128007)\n",
      "\n",
      "\n",
      "(271)\n"
     ]
    }
   ],
   "source": [
    "# example input\n",
    "system_input = \"You are GPT5.\"\n",
    "user_input = \"What is the meaning of life?\"\n",
    "message = [{'role': 'system', 'content': system_input}, {'role': 'user', 'content': user_input}]\n",
    "prompt = tokenizer.apply_chat_template(message, tokenize=False, add_generation_prompt=True)\n",
    "print(prompt)\n",
    "length = len(tokenizer(prompt).input_ids)\n",
    "print(f'token length: {length}')\n",
    "print(f'token_ids: {tokenizer(prompt).input_ids}')\n",
    "for input_id in tokenizer(prompt).input_ids:\n",
    "    print(f'{tokenizer.decode(input_id)}({input_id})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are GPT5.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is the meaning of life?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "I don't know.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "token length: 38\n",
      "token_ids: [128000, 128006, 9125, 128007, 271, 2675, 527, 480, 2898, 20, 13, 128009, 128006, 882, 128007, 271, 3923, 374, 279, 7438, 315, 2324, 30, 128009, 128006, 78191, 128007, 271, 40, 1541, 956, 1440, 13, 128009, 128006, 78191, 128007, 271]\n",
      "<|begin_of_text|>(128000)\n",
      "<|start_header_id|>(128006)\n",
      "system(9125)\n",
      "<|end_header_id|>(128007)\n",
      "\n",
      "\n",
      "(271)\n",
      "You(2675)\n",
      " are(527)\n",
      " G(480)\n",
      "PT(2898)\n",
      "5(20)\n",
      ".(13)\n",
      "<|eot_id|>(128009)\n",
      "<|start_header_id|>(128006)\n",
      "user(882)\n",
      "<|end_header_id|>(128007)\n",
      "\n",
      "\n",
      "(271)\n",
      "What(3923)\n",
      " is(374)\n",
      " the(279)\n",
      " meaning(7438)\n",
      " of(315)\n",
      " life(2324)\n",
      "?(30)\n",
      "<|eot_id|>(128009)\n",
      "<|start_header_id|>(128006)\n",
      "assistant(78191)\n",
      "<|end_header_id|>(128007)\n",
      "\n",
      "\n",
      "(271)\n",
      "I(40)\n",
      " don(1541)\n",
      "'t(956)\n",
      " know(1440)\n",
      ".(13)\n",
      "<|eot_id|>(128009)\n",
      "<|start_header_id|>(128006)\n",
      "assistant(78191)\n",
      "<|end_header_id|>(128007)\n",
      "\n",
      "\n",
      "(271)\n"
     ]
    }
   ],
   "source": [
    "# example input\n",
    "system_input = \"You are GPT5.\"\n",
    "user_input = \"What is the meaning of life?\"\n",
    "assistant_output = \"I don't know.\"\n",
    "message = [{'role': 'system', 'content': system_input}, {'role': 'user', 'content': user_input}, {'role': 'assistant', 'content': assistant_output}]\n",
    "prompt = tokenizer.apply_chat_template(message, tokenize=False, add_generation_prompt=False)\n",
    "print(prompt)\n",
    "length = len(tokenizer(prompt).input_ids)\n",
    "print(f'token length: {length}')\n",
    "print(f'token_ids: {tokenizer(prompt).input_ids}')\n",
    "for input_id in tokenizer(prompt).input_ids:\n",
    "    print(f'{tokenizer.decode(input_id)}({input_id})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立数据集对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "with open('../data/grade_school_math/data/train.jsonl', 'r') as f:\n",
    "    question_data = [json.loads(line) for line in f]\n",
    "\n",
    "with open('generated_data/actor_response_data_01.jsonl', 'r') as f:\n",
    "    actor_data = [json.loads(line) for line in f]\n",
    "\n",
    "with open('generated_data/critic_response_data_02.jsonl', 'r') as f:\n",
    "    critic_data = [json.loads(line) for line in f]\n",
    "\n",
    "with open('generated_data/summarizer_response_data_01.jsonl', 'r') as f:\n",
    "    summarizer_data = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_data_selected = []\n",
    "for datum in actor_data:\n",
    "    if datum['answer_correct'] == datum['answer_actor']:\n",
    "        actor_data_selected.append(datum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate prompts\n",
    "system_prompt = 'You are an actor who is responsible for solving math problems. Given a math problem, you need to give a concise analysis followed by the correct answer in the format \"\\n#### [Answer with digits only]\" in the very end of your response.'\n",
    "messages = [[{'role': 'system', 'content': system_prompt}, \n",
    "             {'role': 'question', 'content': question_data[datum['question_id']]['question']},\n",
    "             {'role': 'assistant', 'content': datum['actor_response']}] for datum in actor_data_selected]\n",
    "input_prompts = [tokenizer.apply_chat_template(message, tokenize=False, add_generation_prompt=False)[:-47] for message in messages]\n",
    "pattern = '<|start_header_id|>assistant<|end_header_id|>\\n\\n'\n",
    "separate_idx = [prompt.find(pattern) + 47 for prompt in input_prompts]\n",
    "instruction_prompts = [prompt[:idx] for prompt, idx in zip(input_prompts, separate_idx)]\n",
    "label_prompts = [prompt[idx:] for prompt, idx in zip(input_prompts, separate_idx)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate instruction_ids\n",
    "instruction_ids = [tokenizer(prompt).input_ids for prompt in instruction_prompts]\n",
    "instruction_token_len = [len(ids) for ids in instruction_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target ids are of the same shape of instruction_ids\n",
    "IGNORE_TOKEN_ID = LabelSmoother.ignore_index # -100\n",
    "target_ids = [[IGNORE_TOKEN_ID] * len(ids) for ids in instruction_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[84, 114, 50, 75, 82, 95, 75, 194, 74, 46]\n"
     ]
    }
   ],
   "source": [
    "# generate label_ids\n",
    "label_ids = [tokenizer(prompt).input_ids for prompt in label_prompts]\n",
    "label_token_len = [len(ids) for ids in label_ids]\n",
    "print(label_token_len[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([58153, 1024])\n",
      "torch.Size([58153, 1024])\n"
     ]
    }
   ],
   "source": [
    "# combine instruction_ids and label_ids into input_ids\n",
    "maxlen = 1024\n",
    "input_ids = [instruction_id + label_id + [tokenizer.pad_token_id] * (maxlen - len(instruction_id) - len(label_id)) for instruction_id, label_id in zip(instruction_ids, label_ids)]\n",
    "# combine target_ids and label_ids into target_ids\n",
    "target_ids = [target_id + label_id + [IGNORE_TOKEN_ID] * (maxlen - len(target_id) - len(label_id)) for target_id, label_id in zip(target_ids, label_ids)]\n",
    "# change list to tensor\n",
    "input_ids = torch.tensor(input_ids)\n",
    "target_ids = torch.tensor(target_ids)\n",
    "print(input_ids.shape)\n",
    "print(target_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([58153, 1024])\n"
     ]
    }
   ],
   "source": [
    "# generate attention_mask\n",
    "attention_mask = input_ids.ne(tokenizer.pad_token_id)\n",
    "print(attention_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine input_ids, labels and attention_mask to achieve the final dataset\n",
    "complete_dataset = dict(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    labels=target_ids,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(torch.device('cuda:0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_ids = input_ids[:4].to(torch.device('cuda:0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_attention_mask = attention_mask[:4].to(torch.device('cuda:0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_label = target_ids[:4].to(torch.device('cuda:0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['loss', 'logits', 'past_key_values'])\n",
      "torch.Size([4, 1024, 128256])\n",
      "tensor(1.3378, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# test model output given input_ids[:4]\n",
    "# release GPU memory\n",
    "with torch.no_grad():\n",
    "    output = model(input_ids=test_input_ids, attention_mask=test_attention_mask, labels=test_label)\n",
    "print(output.keys())\n",
    "print(output.logits.shape)\n",
    "print(output.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save complete_dataset\n",
    "with open('./generated_data/processed_actor_positive_data.pkl', 'wb') as f:\n",
    "    pickle.dump(complete_dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maxlen:1481\n",
      "torch.Size([110133, 1536])\n",
      "torch.Size([110133, 1536])\n",
      "torch.Size([110133, 1536])\n"
     ]
    }
   ],
   "source": [
    "# critic dataset\n",
    "critic_data_selected =  []\n",
    "for datum in critic_data:\n",
    "    if datum['judge_correct'] == datum['judge_critic']:\n",
    "        critic_data_selected.append(datum)\n",
    "system_prompt = 'You are a critic who is responsible for judging the correctness of the actor\\'s answer. Provided with the math problem, correct answer and the student\\'s answer, you need to judge whether the actor\\'s answer is correct. If the actor\\'s answer is right, respond with \"#### The answer is: Accepted.\" Otherwise, analyze the reason why the actor arrived at the wrong answer and respond with \"#### The answer is: Wrong Answer. [Reason for the wrong answer, without displaying the correct number to the question]\".'\n",
    "messages = [[{'role': 'system', 'content': system_prompt},\n",
    "             {'role': 'question', 'content': question_data[actor_data[datum['actor_response_id']]['question_id']]['question']},\n",
    "             {'role': 'correct answer', 'content': question_data[actor_data[datum['actor_response_id']]['question_id']]['answer']},\n",
    "             {'role': 'actor\\'s answer', 'content': actor_data[datum['actor_response_id']]['actor_response']},\n",
    "             {'role': 'assistant', 'content': datum['critic_response']}] for datum in critic_data_selected]\n",
    "input_prompts = [tokenizer.apply_chat_template(message, tokenize=False, add_generation_prompt=False)[:-47] for message in messages]\n",
    "pattern = '<|start_header_id|>assistant<|end_header_id|>\\n\\n'\n",
    "separate_idx = [prompt.find(pattern) + 47 for prompt in input_prompts]\n",
    "instruction_prompts = [prompt[:idx] for prompt, idx in zip(input_prompts, separate_idx)]\n",
    "label_prompts = [prompt[idx:] for prompt, idx in zip(input_prompts, separate_idx)]\n",
    "# generate instruction_ids\n",
    "instruction_ids = [tokenizer(prompt).input_ids for prompt in instruction_prompts]\n",
    "instruction_token_len = [len(ids) for ids in instruction_ids]\n",
    "# target ids are of the same shape of instruction_ids\n",
    "IGNORE_TOKEN_ID = LabelSmoother.ignore_index # -100\n",
    "target_ids = [[IGNORE_TOKEN_ID] * len(ids) for ids in instruction_ids]\n",
    "# generate label_ids\n",
    "label_ids = [tokenizer(prompt).input_ids for prompt in label_prompts]\n",
    "label_token_len = [len(ids) for ids in label_ids]\n",
    "print(f'maxlen:{max([ins+lab for ins, lab in zip(instruction_token_len, label_token_len)])}')\n",
    "# combine instruction_ids and label_ids into input_ids\n",
    "maxlen = 1536\n",
    "input_ids = [instruction_id + label_id + [tokenizer.pad_token_id] * (maxlen - len(instruction_id) - len(label_id)) for instruction_id, label_id in zip(instruction_ids, label_ids)]\n",
    "# combine target_ids and label_ids into target_ids\n",
    "target_ids = [target_id + label_id + [IGNORE_TOKEN_ID] * (maxlen - len(target_id) - len(label_id)) for target_id, label_id in zip(target_ids, label_ids)]\n",
    "# change list to tensor\n",
    "input_ids = torch.tensor(input_ids)\n",
    "target_ids = torch.tensor(target_ids)\n",
    "print(input_ids.shape)\n",
    "print(target_ids.shape)\n",
    "# generate attention_mask\n",
    "attention_mask = input_ids.ne(tokenizer.pad_token_id)\n",
    "print(attention_mask.shape)\n",
    "# combine input_ids, labels and attention_mask to achieve the final dataset\n",
    "complete_dataset = dict(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    labels=target_ids,\n",
    ")\n",
    "# save complete_dataset\n",
    "with open('./generated_data/processed_critic_positive_data.pkl', 'wb') as f:\n",
    "    pickle.dump(complete_dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maxlen:1434\n",
      "torch.Size([86169, 1536])\n",
      "torch.Size([86169, 1536])\n",
      "torch.Size([86169, 1536])\n"
     ]
    }
   ],
   "source": [
    "# summarizer dataset\n",
    "summarizer_data_selected =  []\n",
    "for datum in summarizer_data:\n",
    "    if datum['label_positive']:\n",
    "        summarizer_data_selected.append(datum)\n",
    "\n",
    "system_prompt = 'You are a summarizer who is responsible for deciding the final answer to a given math problem, with the help of an actor\\'s solution and a critic\\'s judgement of whether the actor\\'s answer is correct or not. If the actor\\'s answer is correct, summarize the analysis. Otherwise, fix the actor\\'s answer according to the critic\\'s feedback. Only the correct analysis is allowed to be presented. Do not include statements about whether the actor or critic is correct. Finally, add \"\\n\\n#### [Answer to the question with digits only]\" as a summarization.'\n",
    "messages = [[{'role': 'system', 'content': system_prompt},\n",
    "             {'role': 'question', 'content': question_data[actor_data[critic_data[datum['critic_response_id']]['actor_response_id']]['question_id']]['question']},\n",
    "             {'role': 'actor\\'s answer', 'content': actor_data[critic_data[datum['critic_response_id']]['actor_response_id']]['actor_response']},\n",
    "             {'role': 'critic\\'s judgement', 'content': critic_data[datum['critic_response_id']]['critic_response']},\n",
    "             {'role': 'assistant', 'content': datum['summarizer_response']}] for datum in summarizer_data_selected]\n",
    "input_prompts = [tokenizer.apply_chat_template(message, tokenize=False, add_generation_prompt=False)[:-47] for message in messages]\n",
    "pattern = '<|start_header_id|>assistant<|end_header_id|>\\n\\n'\n",
    "separate_idx = [prompt.find(pattern) + 47 for prompt in input_prompts]\n",
    "instruction_prompts = [prompt[:idx] for prompt, idx in zip(input_prompts, separate_idx)]\n",
    "label_prompts = [prompt[idx:] for prompt, idx in zip(input_prompts, separate_idx)]\n",
    "# generate instruction_ids\n",
    "instruction_ids = [tokenizer(prompt).input_ids for prompt in instruction_prompts]\n",
    "instruction_token_len = [len(ids) for ids in instruction_ids]\n",
    "# target ids are of the same shape of instruction_ids\n",
    "IGNORE_TOKEN_ID = LabelSmoother.ignore_index # -100\n",
    "target_ids = [[IGNORE_TOKEN_ID] * len(ids) for ids in instruction_ids]\n",
    "# generate label_ids\n",
    "label_ids = [tokenizer(prompt).input_ids for prompt in label_prompts]\n",
    "label_token_len = [len(ids) for ids in label_ids]\n",
    "print(f'maxlen:{max([ins+lab for ins, lab in zip(instruction_token_len, label_token_len)])}')\n",
    "# combine instruction_ids and label_ids into input_ids\n",
    "maxlen = 1536\n",
    "input_ids = [instruction_id + label_id + [tokenizer.pad_token_id] * (maxlen - len(instruction_id) - len(label_id)) for instruction_id, label_id in zip(instruction_ids, label_ids)]\n",
    "# combine target_ids and label_ids into target_ids\n",
    "target_ids = [target_id + label_id + [IGNORE_TOKEN_ID] * (maxlen - len(target_id) - len(label_id)) for target_id, label_id in zip(target_ids, label_ids)]\n",
    "# change list to tensor\n",
    "input_ids = torch.tensor(input_ids)\n",
    "target_ids = torch.tensor(target_ids)\n",
    "print(input_ids.shape)\n",
    "print(target_ids.shape)\n",
    "# generate attention_mask\n",
    "attention_mask = input_ids.ne(tokenizer.pad_token_id)\n",
    "print(attention_mask.shape)\n",
    "# combine input_ids, labels and attention_mask to achieve the final dataset\n",
    "complete_dataset = dict(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    labels=target_ids,\n",
    ")\n",
    "# save complete_dataset\n",
    "with open('./generated_data/processed_summarizer_positive_data.pkl', 'wb') as f:\n",
    "    pickle.dump(complete_dataset, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用PEFT+零阶法训练lora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 搭建PEFT模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/hyt/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList, TextIteratorStreamer\n",
    "from threading import Thread\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0,1,2,3,4,5,6,7'\n",
    "import json, pickle, copy\n",
    "from peft import PeftModel\n",
    "from peft import (\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    "    LoraConfig\n",
    ")\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import concurrent.futures\n",
    "from transformers.trainer_pt_utils import LabelSmoother"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.04it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load LLaMA3 model\n",
    "model_pth = '../Meta-Llama-3-8B-Instruct'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_pth,torch_dtype=torch.float16,load_in_4bit=False,load_in_8bit=False)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_pth,torch_dtype=torch.float16,load_in_4bit=False,load_in_8bit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaSdpaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=8,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): LlamaForCausalLM(\n",
      "      (model): LlamaModel(\n",
      "        (embed_tokens): Embedding(128256, 4096)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x LlamaDecoderLayer(\n",
      "            (self_attn): LlamaSdpaAttention(\n",
      "              (q_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "              (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "              (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm()\n",
      "            (post_attention_layernorm): LlamaRMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): LlamaRMSNorm()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.model.layers.0.input_layernorm is converted to float32\n",
      "base_model.model.model.layers.0.post_attention_layernorm is converted to float32\n",
      "base_model.model.model.layers.1.input_layernorm is converted to float32\n",
      "base_model.model.model.layers.1.post_attention_layernorm is converted to float32\n",
      "base_model.model.model.layers.2.input_layernorm is converted to float32\n",
      "base_model.model.model.layers.2.post_attention_layernorm is converted to float32\n",
      "base_model.model.model.layers.3.input_layernorm is converted to float32\n",
      "base_model.model.model.layers.3.post_attention_layernorm is converted to float32\n",
      "base_model.model.model.layers.4.input_layernorm is converted to float32\n",
      "base_model.model.model.layers.4.post_attention_layernorm is converted to float32\n",
      "base_model.model.model.layers.5.input_layernorm is converted to float32\n",
      "base_model.model.model.layers.5.post_attention_layernorm is converted to float32\n",
      "base_model.model.model.layers.6.input_layernorm is converted to float32\n",
      "base_model.model.model.layers.6.post_attention_layernorm is converted to float32\n",
      "base_model.model.model.layers.7.input_layernorm is converted to float32\n",
      "base_model.model.model.layers.7.post_attention_layernorm is converted to float32\n",
      "base_model.model.model.layers.8.input_layernorm is converted to float32\n",
      "base_model.model.model.layers.8.post_attention_layernorm is converted to float32\n",
      "base_model.model.model.layers.9.input_layernorm is converted to float32\n",
      "base_model.model.model.layers.9.post_attention_layernorm is converted to float32\n",
      "base_model.model.model.layers.10.input_layernorm is converted to float32\n",
      "base_model.model.model.layers.10.post_attention_layernorm is converted to float32\n",
      "base_model.model.model.layers.11.input_layernorm is converted to float32\n",
      "base_model.model.model.layers.11.post_attention_layernorm is converted to float32\n",
      "base_model.model.model.layers.12.input_layernorm is converted to float32\n",
      "base_model.model.model.layers.12.post_attention_layernorm is converted to float32\n",
      "base_model.model.model.layers.13.input_layernorm is converted to float32\n",
      "base_model.model.model.layers.13.post_attention_layernorm is converted to float32\n",
      "base_model.model.model.layers.14.input_layernorm is converted to float32\n",
      "base_model.model.model.layers.14.post_attention_layernorm is converted to float32\n",
      "base_model.model.model.layers.15.input_layernorm is converted to float32\n",
      "base_model.model.model.layers.15.post_attention_layernorm is converted to float32\n",
      "base_model.model.model.layers.16.input_layernorm is converted to float32\n",
      "base_model.model.model.layers.16.post_attention_layernorm is converted to float32\n",
      "base_model.model.model.layers.17.input_layernorm is converted to float32\n",
      "base_model.model.model.layers.17.post_attention_layernorm is converted to float32\n",
      "base_model.model.model.layers.18.input_layernorm is converted to float32\n",
      "base_model.model.model.layers.18.post_attention_layernorm is converted to float32\n",
      "base_model.model.model.layers.19.input_layernorm is converted to float32\n",
      "base_model.model.model.layers.19.post_attention_layernorm is converted to float32\n",
      "base_model.model.model.layers.20.input_layernorm is converted to float32\n",
      "base_model.model.model.layers.20.post_attention_layernorm is converted to float32\n",
      "base_model.model.model.layers.21.input_layernorm is converted to float32\n",
      "base_model.model.model.layers.21.post_attention_layernorm is converted to float32\n",
      "base_model.model.model.layers.22.input_layernorm is converted to float32\n",
      "base_model.model.model.layers.22.post_attention_layernorm is converted to float32\n",
      "base_model.model.model.layers.23.input_layernorm is converted to float32\n",
      "base_model.model.model.layers.23.post_attention_layernorm is converted to float32\n",
      "base_model.model.model.layers.24.input_layernorm is converted to float32\n",
      "base_model.model.model.layers.24.post_attention_layernorm is converted to float32\n",
      "base_model.model.model.layers.25.input_layernorm is converted to float32\n",
      "base_model.model.model.layers.25.post_attention_layernorm is converted to float32\n",
      "base_model.model.model.layers.26.input_layernorm is converted to float32\n",
      "base_model.model.model.layers.26.post_attention_layernorm is converted to float32\n",
      "base_model.model.model.layers.27.input_layernorm is converted to float32\n",
      "base_model.model.model.layers.27.post_attention_layernorm is converted to float32\n",
      "base_model.model.model.layers.28.input_layernorm is converted to float32\n",
      "base_model.model.model.layers.28.post_attention_layernorm is converted to float32\n",
      "base_model.model.model.layers.29.input_layernorm is converted to float32\n",
      "base_model.model.model.layers.29.post_attention_layernorm is converted to float32\n",
      "base_model.model.model.layers.30.input_layernorm is converted to float32\n",
      "base_model.model.model.layers.30.post_attention_layernorm is converted to float32\n",
      "base_model.model.model.layers.31.input_layernorm is converted to float32\n",
      "base_model.model.model.layers.31.post_attention_layernorm is converted to float32\n",
      "base_model.model.model.norm is converted to float32\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.named_modules():\n",
    "    if \"norm\" in name:\n",
    "        module = module.to(torch.float32)\n",
    "        print(f'{name} is converted to float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight torch.Size([1024, 8])\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 8])\n",
      "base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight torch.Size([1024, 8])\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 8])\n",
      "base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight torch.Size([1024, 8])\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 8])\n",
      "base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight torch.Size([1024, 8])\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 8])\n",
      "base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight torch.Size([1024, 8])\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 8])\n",
      "base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight torch.Size([1024, 8])\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 8])\n",
      "base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight torch.Size([1024, 8])\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 8])\n",
      "base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight torch.Size([1024, 8])\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 8])\n",
      "base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight torch.Size([1024, 8])\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 8])\n",
      "base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight torch.Size([1024, 8])\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 8])\n",
      "base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight torch.Size([1024, 8])\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 8])\n",
      "base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight torch.Size([1024, 8])\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 8])\n",
      "base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight torch.Size([1024, 8])\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 8])\n",
      "base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight torch.Size([1024, 8])\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 8])\n",
      "base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight torch.Size([1024, 8])\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 8])\n",
      "base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight torch.Size([1024, 8])\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 8])\n",
      "base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight torch.Size([1024, 8])\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 8])\n",
      "base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight torch.Size([1024, 8])\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 8])\n",
      "base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight torch.Size([1024, 8])\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 8])\n",
      "base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight torch.Size([1024, 8])\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 8])\n",
      "base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight torch.Size([1024, 8])\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 8])\n",
      "base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight torch.Size([1024, 8])\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 8])\n",
      "base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight torch.Size([1024, 8])\n",
      "base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 8])\n",
      "base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight torch.Size([1024, 8])\n",
      "base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 8])\n",
      "base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight torch.Size([1024, 8])\n",
      "base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 8])\n",
      "base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight torch.Size([1024, 8])\n",
      "base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 8])\n",
      "base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight torch.Size([1024, 8])\n",
      "base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 8])\n",
      "base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight torch.Size([1024, 8])\n",
      "base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 8])\n",
      "base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.28.self_attn.k_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.weight torch.Size([1024, 8])\n",
      "base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 8])\n",
      "base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.29.self_attn.k_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.weight torch.Size([1024, 8])\n",
      "base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 8])\n",
      "base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.30.self_attn.k_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.weight torch.Size([1024, 8])\n",
      "base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 8])\n",
      "base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.31.self_attn.k_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.weight torch.Size([1024, 8])\n",
      "base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 8])\n",
      "base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "Total trainable parameters: 6815744\n"
     ]
    }
   ],
   "source": [
    "# check trainable parameters in model\n",
    "trainable_parameters = [(name, parameters) for name, parameters in model.named_parameters() if parameters.requires_grad]\n",
    "for name, parameters in trainable_parameters:\n",
    "    print(name, parameters.shape)\n",
    "print(f'Total trainable parameters: {sum([parameters.numel() for _, parameters in trainable_parameters])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128256, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaSdpaAttention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "              (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "              (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(torch.device('cuda:0'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试零阶训练代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iters = 10\n",
    "scaling_factor = 1\n",
    "zo_eps = 1e-3\n",
    "batch_size = 2\n",
    "random_seed = 123456\n",
    "learning_rate = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0, loss1: 0.9620891809463501\n",
      "iter: 0, loss2: 0.9620891809463501\n",
      "iter: 1, loss1: 1.249617576599121\n",
      "iter: 1, loss2: 1.249617576599121\n",
      "iter: 2, loss1: 0.9707486033439636\n",
      "iter: 2, loss2: 0.9707486033439636\n",
      "iter: 3, loss1: 1.5380691289901733\n",
      "iter: 3, loss2: 1.5380691289901733\n",
      "iter: 4, loss1: 1.1894009113311768\n",
      "iter: 4, loss2: 1.1894009113311768\n",
      "iter: 5, loss1: 1.4856212139129639\n",
      "iter: 5, loss2: 1.4856212139129639\n",
      "iter: 6, loss1: 0.9410967230796814\n",
      "iter: 6, loss2: 0.9410967230796814\n",
      "iter: 7, loss1: 1.1143782138824463\n",
      "iter: 7, loss2: 1.1143782138824463\n",
      "iter: 8, loss1: 1.1747761964797974\n",
      "iter: 8, loss2: 1.1747761964797974\n",
      "iter: 9, loss1: 0.9255077838897705\n",
      "iter: 9, loss2: 0.9255077838897705\n"
     ]
    }
   ],
   "source": [
    "with open('./generated_data/processed_actor_positive_data.pkl', 'rb') as f:\n",
    "    complete_dataset = pickle.load(f)\n",
    "    \n",
    "shuffled_ids = torch.randperm(len(complete_dataset['input_ids']))\n",
    "\n",
    "for iter in range(10):\n",
    "\n",
    "    # sample data batch\n",
    "    batched_input_ids = complete_dataset['input_ids'][shuffled_ids[iter * batch_size: (iter + 1) * batch_size]].to(torch.device('cuda:0'))\n",
    "    batched_labels = complete_dataset['labels'][shuffled_ids[iter * batch_size: (iter + 1) * batch_size]].to(torch.device('cuda:0'))\n",
    "    batched_attention_mask = complete_dataset['attention_mask'][shuffled_ids[iter * batch_size: (iter + 1) * batch_size]].to(torch.device('cuda:0'))\n",
    "    \n",
    "    # random perturbation\n",
    "    torch.manual_seed(random_seed)\n",
    "    for _, params in trainable_parameters:\n",
    "        z = torch.normal(mean=0, std=1, size=params.data.size(), device=params.data.device, dtype=params.data.dtype)\n",
    "        params.data += scaling_factor * z * zo_eps\n",
    "    \n",
    "    # loss computation\n",
    "    with torch.no_grad():\n",
    "        loss1 = model(input_ids=batched_input_ids, attention_mask=batched_attention_mask, labels=batched_labels).loss\n",
    "        print(f'iter: {iter}, loss1: {loss1}')\n",
    "\n",
    "    # perturbate in the opposite direction\n",
    "    torch.manual_seed(random_seed)\n",
    "    for _, params in trainable_parameters:\n",
    "        z = torch.normal(mean=0, std=1, size=params.data.size(), device=params.data.device, dtype=params.data.dtype)\n",
    "        params.data -= 2 * scaling_factor * z * zo_eps\n",
    "\n",
    "    # loss computation\n",
    "    with torch.no_grad():\n",
    "        loss2 = model(input_ids=batched_input_ids, attention_mask=batched_attention_mask, labels=batched_labels).loss\n",
    "        print(f'iter: {iter}, loss2: {loss2}')\n",
    "    \n",
    "    # compute update\n",
    "    projected_grad = ((loss1 - loss2) / (2 * zo_eps)).item()\n",
    "\n",
    "    # reset and update parameters\n",
    "    torch.manual_seed(random_seed)\n",
    "    for _, params in trainable_parameters:\n",
    "        z = torch.normal(mean=0, std=1, size=params.data.size(), device=params.data.device, dtype=params.data.dtype)\n",
    "        params.data += scaling_factor * (zo_eps - projected_grad * learning_rate) * z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实现多卡并行的零阶法LoRA微调"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型与数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/hyt/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList, TextIteratorStreamer\n",
    "from threading import Thread\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0,1,2,3,4,5,6,7'\n",
    "import json, pickle, copy\n",
    "from peft import PeftModel\n",
    "from peft import (\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    "    LoraConfig\n",
    ")\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import concurrent.futures\n",
    "from transformers.trainer_pt_utils import LabelSmoother"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.59it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load LLaMA3 model\n",
    "model_pth = '../Meta-Llama-3-8B-Instruct'\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_pth,torch_dtype=torch.float32,load_in_4bit=False,load_in_8bit=False)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=8,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "models = [copy.deepcopy(model).to(torch.device(f'cuda:{_}')) for _ in range(8)]\n",
    "tokenizers = [AutoTokenizer.from_pretrained(model_pth,torch_dtype=torch.float16,load_in_4bit=False,load_in_8bit=False) for _ in range(8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    model.trainable_parameters = [(name, parameters) for name, parameters in model.named_parameters() if parameters.requires_grad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tokenizer in tokenizers:\n",
    "    tokenizer.pad_token = tokenizer.bos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./generated_data/processed_actor_positive_data.pkl', 'rb') as f:\n",
    "    complete_dataset = pickle.load(f)\n",
    "item_counts = [complete_dataset['labels'][id].ne(-100).sum().item() for id in range(complete_dataset['labels'].size(0))]\n",
    "complete_dataset['item_counts'] = item_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 单卡Loss计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_single_node(model, input_ids, labels, attention_mask, zo_eps, random_seed, batch_size, scale_factor, device, reset=False):\n",
    "\n",
    "    n_data = len(input_ids)\n",
    "    n_batch = n_data // batch_size\n",
    "\n",
    "    # random perturbation\n",
    "    generator = torch.Generator().manual_seed(random_seed)\n",
    "    for _, params in model.trainable_parameters:\n",
    "        z = torch.normal(mean=0, std=1, size=params.data.size(), generator=generator, dtype=params.data.dtype).to(device=device)\n",
    "        params.data += z * zo_eps\n",
    "    \n",
    "    # compute loss1\n",
    "    loss1 = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_id in range(n_batch):\n",
    "            loss1 += model(input_ids=input_ids[batch_id * batch_size: (batch_id + 1) * batch_size].to(device),\n",
    "                           labels=labels[batch_id * batch_size: (batch_id + 1) * batch_size].to(device),\n",
    "                           attention_mask=attention_mask[batch_id * batch_size: (batch_id + 1) * batch_size].to(device)).loss.item()\n",
    "    \n",
    "    # perturbate in the opposite direction\n",
    "    generator = generator.manual_seed(random_seed)\n",
    "    for _, params in model.trainable_parameters:\n",
    "        z = torch.normal(mean=0, std=1, size=params.data.size(), generator=generator, dtype=params.data.dtype).to(device=device)\n",
    "        params.data -= z * zo_eps * 2\n",
    "    \n",
    "    # compute loss2\n",
    "    loss2 = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_id in range(n_batch):\n",
    "            loss2 += model(input_ids=input_ids[batch_id * batch_size: (batch_id + 1) * batch_size].to(device),\n",
    "                           labels=labels[batch_id * batch_size: (batch_id + 1) * batch_size].to(device),\n",
    "                           attention_mask=attention_mask[batch_id * batch_size: (batch_id + 1) * batch_size].to(device)).loss.item()\n",
    "    \n",
    "    if reset:\n",
    "        generator = generator.manual_seed(random_seed)\n",
    "        for _, params in model.trainable_parameters:\n",
    "            z = torch.normal(mean=0, std=1, size=params.data.size(), generator=generator, dtype=params.data.dtype).to(device=device)\n",
    "            params.data += z * zo_eps\n",
    "    \n",
    "    return loss1 * scale_factor, loss2 * scale_factor # the scale_factor is corresponding to the number of minibatches in the computation of the CE loss (i.e., number of non-[-100] labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多卡Loss计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_multi_node(models, input_ids, labels, attention_mask, zo_eps, random_seed, batch_size, item_counts=None, n_GPU=8, reset=False): # batch_size is exactly the batch_size per node\n",
    "\n",
    "    n_data = len(input_ids)\n",
    "    n_data_per_GPU = n_data // n_GPU\n",
    "\n",
    "    if item_counts is None:\n",
    "        item_counts = [labels[id].ne(-100).sum().item() for id in range(labels.size(0))]\n",
    "    \n",
    "    scale_factors = [sum(item_counts[i * n_data_per_GPU: (i+1) * n_data_per_GPU]) for i in range(n_GPU)]\n",
    "    loss1, loss2 = 0.0, 0.0\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        futures = []\n",
    "        for i in range(n_GPU):\n",
    "            futures.append(executor.submit(compute_loss_single_node, models[i], \n",
    "                                           input_ids[i * n_data_per_GPU: (i+1) * n_data_per_GPU], \n",
    "                                           labels[i * n_data_per_GPU: (i+1) * n_data_per_GPU], \n",
    "                                           attention_mask[i * n_data_per_GPU: (i+1) * n_data_per_GPU],\n",
    "                                           zo_eps, random_seed, batch_size, scale_factors[i],\n",
    "                                           torch.device(f'cuda:{i}'), reset))\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            loss1_delta, loss2_delta = future.result()\n",
    "            loss1 += loss1_delta\n",
    "            loss2 += loss2_delta\n",
    "    \n",
    "    total_scale_factor = sum(scale_factors)\n",
    "    return loss1 / total_scale_factor, loss2 / total_scale_factor   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 单卡参数更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_params_single_node(model, zo_eps, random_seed, scale_factor, device, reset=False): # scale factor here denotes the gradient step\n",
    "    \n",
    "    generator = torch.Generator().manual_seed(random_seed)\n",
    "    for _, params in model.trainable_parameters:\n",
    "        z = torch.normal(mean=0, std=1, size=params.data.size(), generator=generator, dtype=params.data.dtype).to(device=device)\n",
    "        params.data += z * (zo_eps - scale_factor) if not reset else z * (-scale_factor)\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多卡并行更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_params_multi_node(models, zo_eps, random_seed, scale_factor, n_GPU=8, reset=False):\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        futures = []\n",
    "        for i in range(n_GPU):\n",
    "            futures.append(executor.submit(update_params_single_node, models[i], zo_eps, random_seed, scale_factor, torch.device(f'cuda:{i}'), reset))\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            future.result()\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多卡零阶LoRA微调"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zo_lora_finetuning_multi_node(models, dataset, zo_eps, learning_rate, batch_size, random_seeds=None, n_iters=None, n_epochs=None, n_GPU=8, verbose=False):\n",
    "    \n",
    "    assert (n_iters is None) ^ (n_epochs is None)\n",
    "    input_ids, labels, attention_mask, item_counts = dataset['input_ids'], dataset['labels'], dataset['attention_mask'], dataset['item_counts']\n",
    "    \n",
    "    n_iters_per_epoch = len(input_ids) // batch_size\n",
    "    n_iters = n_iters_per_epoch * n_epochs if n_epochs is not None else n_iters\n",
    "    batch_size_per_GPU = batch_size // n_GPU\n",
    "\n",
    "    if random_seeds is None:\n",
    "        random_seeds = torch.randint(100000000, (n_iters,))\n",
    "\n",
    "    e = 0 # epoch\n",
    "    iter = 0 # iteration\n",
    "    while iter < n_iters:\n",
    "\n",
    "        # shuffle data for epoch e\n",
    "        shuffled_ids = torch.randperm(len(input_ids))\n",
    "\n",
    "        for i in range(n_iters_per_epoch):\n",
    "\n",
    "            if iter >= n_iters:\n",
    "                break\n",
    "\n",
    "            # get iter data\n",
    "            batch_input_ids = input_ids[shuffled_ids[i * batch_size: (i+1) * batch_size]]\n",
    "            batch_labels = labels[shuffled_ids[i * batch_size: (i+1) * batch_size]]\n",
    "            batch_attention_mask = attention_mask[shuffled_ids[i * batch_size: (i+1) * batch_size]]\n",
    "            batch_item_counts = [item_counts[shuffled_ids[j]] for j in range(i * batch_size, (i+1) * batch_size)]\n",
    "\n",
    "            # compute loss1, loss2\n",
    "            loss1, loss2 = compute_loss_multi_node(models=models, \n",
    "                                                   input_ids=batch_input_ids, \n",
    "                                                   labels=batch_labels, \n",
    "                                                   attention_mask=batch_attention_mask, \n",
    "                                                   zo_eps=zo_eps, \n",
    "                                                   random_seed=random_seeds[iter].item(),\n",
    "                                                   batch_size=batch_size_per_GPU,\n",
    "                                                   item_counts=batch_item_counts,\n",
    "                                                   n_GPU=n_GPU,\n",
    "                                                   reset=False)\n",
    "            \n",
    "            # compute update stepsize\n",
    "            scale_factor = (loss1 - loss2) / (2 * zo_eps) * learning_rate\n",
    "\n",
    "            # update params\n",
    "            update_params_multi_node(models=models, \n",
    "                                     zo_eps=zo_eps, \n",
    "                                     random_seed=random_seeds[iter].item(),\n",
    "                                     scale_factor=scale_factor,\n",
    "                                     n_GPU=n_GPU,\n",
    "                                     reset=False)\n",
    "\n",
    "            if verbose:\n",
    "                print(f'In epoch {e} iter {i}, loss1={loss1}, loss2={loss2}')\n",
    "            iter += 1\n",
    "        \n",
    "        e += 1\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch 0 iter 0, loss1=1.245854886653261, loss2=1.2432450466212233\n",
      "In epoch 0 iter 1, loss1=1.26297794536308, loss2=1.2639132856709476\n",
      "In epoch 0 iter 2, loss1=1.2700721938170232, loss2=1.2708157269087872\n",
      "In epoch 0 iter 3, loss1=1.220146984956842, loss2=1.2196024243648236\n",
      "In epoch 0 iter 4, loss1=1.2793298186567978, loss2=1.2790694497986326\n",
      "In epoch 0 iter 5, loss1=1.290576580576764, loss2=1.292582733537844\n",
      "In epoch 0 iter 6, loss1=1.233810990897503, loss2=1.2348346873733866\n",
      "In epoch 0 iter 7, loss1=1.255271207099683, loss2=1.2543755417670597\n",
      "In epoch 0 iter 8, loss1=1.2425236751705417, loss2=1.2442799435157708\n",
      "In epoch 0 iter 9, loss1=1.2452937210054018, loss2=1.2426205011298876\n"
     ]
    }
   ],
   "source": [
    "zo_eps = 1e-3\n",
    "learning_rate = 1e-6\n",
    "\n",
    "zo_lora_finetuning_multi_node(models=models,\n",
    "                              dataset=complete_dataset,\n",
    "                              zo_eps=zo_eps,\n",
    "                              learning_rate=learning_rate,\n",
    "                              batch_size=128,\n",
    "                              random_seeds=None,\n",
    "                              n_iters=10,\n",
    "                              n_epochs=None,\n",
    "                              n_GPU=8,\n",
    "                              verbose=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "tensor(0.0043, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "False\n",
      "tensor(0.0043, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "False\n",
      "tensor(0.0043, device='cuda:2', grad_fn=<MaxBackward1>)\n",
      "False\n",
      "tensor(0.0043, device='cuda:3', grad_fn=<MaxBackward1>)\n",
      "False\n",
      "tensor(0.0043, device='cuda:4', grad_fn=<MaxBackward1>)\n",
      "False\n",
      "tensor(0.0043, device='cuda:5', grad_fn=<MaxBackward1>)\n",
      "False\n",
      "tensor(0.0043, device='cuda:6', grad_fn=<MaxBackward1>)\n",
      "False\n",
      "tensor(0.0043, device='cuda:7', grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "zo_eps = 1e-3\n",
    "params = [model.base_model.model.model.layers[31].self_attn.k_proj.lora_A.default.weight.clone() for model in models]\n",
    "compute_loss_multi_node(models=models,\n",
    "                        input_ids=complete_dataset['input_ids'][:8],\n",
    "                        labels=complete_dataset['labels'][:8],\n",
    "                        attention_mask=complete_dataset['attention_mask'][:8],\n",
    "                        zo_eps=zo_eps,\n",
    "                        random_seed=123456,\n",
    "                        batch_size=1,\n",
    "                        item_counts=complete_dataset['item_counts'][:8],\n",
    "                        n_GPU=8,\n",
    "                        reset=False)\n",
    "update_params_multi_node(models=models,\n",
    "                         zo_eps=zo_eps,\n",
    "                         random_seed=123456,\n",
    "                         scale_factor=0,\n",
    "                         n_GPU=8,\n",
    "                         reset=False)\n",
    "params_new = [model.base_model.model.model.layers[31].self_attn.k_proj.lora_A.default.weight.clone() for model in models]\n",
    "\n",
    "for param, param_new in zip(params, params_new):\n",
    "    print(torch.allclose(param, param_new))\n",
    "    print(torch.abs(param - param_new).max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "tensor(1.8626e-09, device='cuda:0', grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "zo_eps = 1\n",
    "params0 = models[0].base_model.model.model.layers[31].self_attn.k_proj.lora_A.default.weight.clone()\n",
    "\n",
    "compute_loss_single_node(model=models[0],\n",
    "                        input_ids=complete_dataset['input_ids'][:1],\n",
    "                        labels=complete_dataset['labels'][:1],\n",
    "                        attention_mask=complete_dataset['attention_mask'][:1],\n",
    "                        zo_eps=zo_eps,\n",
    "                        random_seed=123456,\n",
    "                        batch_size=1,\n",
    "                        scale_factor=0,\n",
    "                        device=torch.device('cuda:0'),\n",
    "                        reset=False)\n",
    "update_params_single_node(model=models[0],\n",
    "                            zo_eps=zo_eps,\n",
    "                            random_seed=123456,\n",
    "                            scale_factor=0,\n",
    "                            device=torch.device('cuda:0'),\n",
    "                            reset=False)\n",
    "params0_new = models[0].base_model.model.model.layers[31].self_attn.k_proj.lora_A.default.weight.clone()\n",
    "\n",
    "print(torch.allclose(params0, params0_new))\n",
    "print(torch.abs(params0 - params0_new).max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "tensor(4.7684e-07, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "False\n",
      "tensor(1.1921e-07, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "False\n",
      "tensor(1.1921e-07, device='cuda:2', grad_fn=<MaxBackward1>)\n",
      "False\n",
      "tensor(1.1921e-07, device='cuda:3', grad_fn=<MaxBackward1>)\n",
      "False\n",
      "tensor(1.1921e-07, device='cuda:4', grad_fn=<MaxBackward1>)\n",
      "False\n",
      "tensor(1.1921e-07, device='cuda:5', grad_fn=<MaxBackward1>)\n",
      "False\n",
      "tensor(1.1921e-07, device='cuda:6', grad_fn=<MaxBackward1>)\n",
      "False\n",
      "tensor(1.1921e-07, device='cuda:7', grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "zo_eps = 1\n",
    "params = [model.base_model.model.model.layers[31].self_attn.k_proj.lora_A.default.weight.clone() for model in models]\n",
    "\n",
    "update_params_multi_node(models=models,\n",
    "                         zo_eps=zo_eps,\n",
    "                         random_seed=123456,\n",
    "                         scale_factor=0,\n",
    "                         n_GPU=8,\n",
    "                         reset=False)\n",
    "\n",
    "update_params_multi_node(models=models,\n",
    "                         zo_eps=zo_eps,\n",
    "                         random_seed=123456,\n",
    "                         scale_factor=3,\n",
    "                         n_GPU=8,\n",
    "                         reset=False)\n",
    "\n",
    "update_params_multi_node(models=models,\n",
    "                         zo_eps=zo_eps,\n",
    "                         random_seed=123456,\n",
    "                         scale_factor=0,\n",
    "                         n_GPU=8,\n",
    "                         reset=False)\n",
    "params_new = [model.base_model.model.model.layers[31].self_attn.k_proj.lora_A.default.weight.clone() for model in models]\n",
    "\n",
    "for param, param_new in zip(params, params_new):\n",
    "    print(torch.allclose(param, param_new))\n",
    "    print(torch.abs(param - param_new).max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(False, device='cuda:0')\n",
      "tensor(3.4791, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(3.4791, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(2.3842e-07, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "params1: tensor([-0.4380, -0.2725,  1.4078,  ..., -1.3894, -0.0833,  1.6678],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "params2: tensor([ 0.0876, -0.0185, -1.0319,  ..., -1.4705,  0.1747,  3.3763],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "params3: tensor([-0.9637, -0.5265,  3.8476,  ..., -1.3083, -0.3413, -0.0408],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "params4: tensor([-0.4380, -0.2725,  1.4078,  ..., -1.3894, -0.0833,  1.6678],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "random_seed = 123456\n",
    "params1 = models[0].base_model.model.model.layers[31].self_attn.k_proj.lora_A.default.weight.clone()\n",
    "torch.manual_seed(random_seed)\n",
    "for _, params in models[0].trainable_parameters:\n",
    "    z = torch.normal(mean=0, std=1, size=params.data.size(), device=torch.device('cuda:0'), dtype=params.data.dtype)\n",
    "    params.data += z * zo_eps\n",
    "params2 = models[0].base_model.model.model.layers[31].self_attn.k_proj.lora_A.default.weight.clone()\n",
    "torch.manual_seed(random_seed)\n",
    "for _, params in models[0].trainable_parameters:\n",
    "    z = torch.normal(mean=0, std=1, size=params.data.size(), device=torch.device('cuda:0'), dtype=params.data.dtype)\n",
    "    params.data -= z * zo_eps * 2 \n",
    "params3 = models[0].base_model.model.model.layers[31].self_attn.k_proj.lora_A.default.weight.clone()\n",
    "torch.manual_seed(random_seed)\n",
    "for _, params in models[0].trainable_parameters:\n",
    "    z = torch.normal(mean=0, std=1, size=params.data.size(), device=torch.device('cuda:0'), dtype=params.data.dtype)\n",
    "    params.data += z * zo_eps\n",
    "# update_params_single_node(models[0], zo_eps, random_seed, 0, torch.device('cuda:0'), reset=True)\n",
    "params4 = models[0].base_model.model.model.layers[31].self_attn.k_proj.lora_A.default.weight.clone()\n",
    "\n",
    "print((params1[0] == params2[0]).all())\n",
    "print((params1[0] - params2[0]).abs().max())\n",
    "print((params1[0] == params3[0]).all())\n",
    "print((params1[0] - params3[0]).abs().max())\n",
    "print((params1[0] == params4[0]).all())\n",
    "print((params1[0] - params4[0]).abs().max())\n",
    "\n",
    "print(f'params1: {params1[0]}')\n",
    "print(f'params2: {params2[0]}')\n",
    "print(f'params3: {params3[0]}')\n",
    "print(f'params4: {params4[0]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hyt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
