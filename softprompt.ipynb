{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gradio'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgr\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM, AutoTokenizer\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gradio'"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList, TextIteratorStreamer\n",
    "from threading import Thread\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0,1,2,3'\n",
    "import torch.nn as nn\n",
    "import inspect\n",
    "import torch.multiprocessing as mp\n",
    "mp.set_start_method('spawn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.96it/s]\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/blog/llama2#using-transformers\n",
    "\n",
    "\n",
    "tiny_llama = \"/mnt/xue.w/models/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/77e23968eed12d195bd46c519aa679cc22a27ddc\"\n",
    "llama_7b_hf_chat = \"/mnt/xue.w/models/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/c1b0db933684edbfe29a06fa47eb19cc48025e93\"\n",
    "# code_llama_70 = '/mnt/xue.w/models/hub/models--codellama--CodeLlama-70b-hf/snapshots/4570a4edc524fb9f20f605b417bb43828fa5997a'\n",
    "\n",
    "miqu_70b ='/mnt/xue.w/models/hub/models--miqudev--miqu-1-70b/models--miqudev--miqu-1-70b/snapshots/82f0daa6767263aa5990dea54dbb13e94d096de7'\n",
    "Mixtral_8x7b_instruct ='/mnt/xue.w/models/hub/models--mistralai--Mixtral-8x7B-Instruct-v0.1/snapshots/125c431e2ff41a156b9f9076f744d2f35dd6e67a'\n",
    "\n",
    "model_pth = llama_7b_hf_chat\n",
    "\n",
    "# setting softprompt model wrapper\n",
    "# class SoftPromptWrapper():\n",
    "#     def __init__(self, model):\n",
    "#         super().__init__()\n",
    "#         self.model = model\n",
    "#         self.soft_prompt = None\n",
    "\n",
    "#     def set_soft_prompt(self, soft_prompt):\n",
    "#         self.soft_prompt = soft_prompt\n",
    "\n",
    "#     def forward(self, input_ids, **kwargs):\n",
    "#         if self.soft_prompt is not None:\n",
    "#             input_embeds = self.model.gt_input_embeddings()(input_ids)  \n",
    "#             input_embeds = torch.cat([input_embeds, self.soft_prompt], dim=1)\n",
    "#             input_ids = None\n",
    "#             kwargs[\"inputs_embeds\"] = input_embeds\n",
    "#         return self.model(input_ids, **kwargs)\n",
    "    \n",
    "#     def generate(self, **kwargs):\n",
    "#         # add softprompt to kwargs\n",
    "#         if self.soft_prompt is not None:\n",
    "#             if \"input_ids\" in kwargs:\n",
    "#                 input_embeds = self.model.get_input_embeddings()(kwargs[\"input_ids\"])  \n",
    "#                 input_embeds = torch.cat([input_embeds, self.soft_prompt], dim=1)\n",
    "#                 kwargs[\"inputs_embeds\"] = input_embeds\n",
    "#                 kwargs[\"input_ids\"] = None\n",
    "#             else:\n",
    "#                 kwargs[\"inputs_embeds\"] = self.soft_prompt\n",
    "#         return self.model.generate(**kwargs)\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "        \n",
    "# Loading the tokenizer and model from local path.\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_pth,torch_dtype=torch.float16,device_map=\"auto\",load_in_4bit=False,load_in_8bit=False)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_pth,torch_dtype=torch.float16,device_map=\"auto\",load_in_4bit=False,load_in_8bit=False)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_pth,torch_dtype=torch.float16,load_in_4bit=False,load_in_8bit=False)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_pth,torch_dtype=torch.float16,load_in_4bit=False,load_in_8bit=False)\n",
    "\n",
    "# model = SoftPromptWrapper(model)\n",
    "\n",
    "# get token embedding dimension\n",
    "# embed_dim = model.model.get_input_embeddings().embedding_dim\n",
    "# print(f\"Embedding dimension: {embed_dim}\")    # 4096\n",
    "\n",
    "# set soft prompt\n",
    "soft_prompt = model.get_input_embeddings()(tokenizer(\"In this conversation, the user is asking the assistant for help with a coding problem. The assistant should provide a helpful response.\", return_tensors=\"pt\").input_ids)\n",
    "# model.set_soft_prompt(soft_prompt)\n",
    "\n",
    "# using CUDA for an optimal experience\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        stop_ids = [2]  # IDs of tokens where the generation should stop.\n",
    "        for stop_id in stop_ids:\n",
    "            if input_ids[0][-1] == stop_id:  # Checking if the last generated token is a stop token.\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "class BatchedStopOnTokens(StoppingCriteria):\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        stop_ids = [2]  # IDs of tokens where the generation should stop.\n",
    "        for stop_id in stop_ids:\n",
    "            # checking if stop token appears in every batch (not just the last token)\n",
    "            if (input_ids == stop_id).any(dim=-1).all():\n",
    "                return True\n",
    "            # check if stop token is generated in all batches\n",
    "            # if all([input_id[-1] == stop_id for input_id in input_ids]):\n",
    "            #     return True\n",
    "        return False\n",
    "\n",
    "\n",
    "# Function to generate model predictions.\n",
    "def predict(message,history = []):\n",
    "    history_transformer_format = history + [[message, \"\"]]\n",
    "    stop = StopOnTokens()\n",
    "\n",
    "    # Formatting the input for the model.\n",
    "    messages = \"</s>\".join([\"</s>\".join([\"\\n<|user|>:\" + item[0], \"\\n<|assistant|>:\" + item[1]])\n",
    "                        for item in history_transformer_format])\n",
    "    model_inputs = tokenizer([messages], return_tensors=\"pt\")\n",
    "    # .to(device)\n",
    "    streamer = TextIteratorStreamer(tokenizer, timeout=10., skip_prompt=True, skip_special_tokens=True)\n",
    "    generate_kwargs = dict(\n",
    "        model_inputs,\n",
    "        streamer=streamer,\n",
    "        max_new_tokens=1024,\n",
    "        do_sample=True,\n",
    "        top_p=0.95,\n",
    "        top_k=50,\n",
    "        temperature=0.7,\n",
    "        num_beams=1,\n",
    "        stopping_criteria=StoppingCriteriaList([stop])\n",
    "    )\n",
    "    t = Thread(target=model.generate, kwargs=generate_kwargs)\n",
    "    t.start()  # Starting the generation in a separate thread.\n",
    "    partial_message = \"\"\n",
    "    for new_token in streamer:\n",
    "        partial_message += new_token\n",
    "        if '</s>' in partial_message:  # Breaking the loop if the stop token is generated.\n",
    "            break\n",
    "    yield partial_message\n",
    "    # return partial_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# softprompt training in gsm8k dataset\n",
    "import re\n",
    "import json\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm\n",
    "embedding_dim = 4096\n",
    "\n",
    "ANS_RE = re.compile(r\"#### (\\-?[0-9\\.\\,]+)\")\n",
    "def extract_answer(completion):\n",
    "    match = ANS_RE.search(completion)\n",
    "    if match:\n",
    "        match_str = match.group(1).strip()\n",
    "        match_str = match_str.replace(\",\", \"\")\n",
    "        return match_str\n",
    "    else:\n",
    "        return \"[invalid]\"\n",
    "\n",
    "def compute_loss(model, tokenizer, sys_prompt, soft_prompt, data, device):\n",
    "\n",
    "    batch_size = len(data)\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for datum in data:\n",
    "            question, answer = datum['question'], datum['answer']\n",
    "            streamer = TextIteratorStreamer(tokenizer, timeout=10., skip_prompt=True, skip_special_tokens=True)\n",
    "            stop = StopOnTokens()\n",
    "            input_prompt = \"<|system|>:\" + sys_prompt + \"</s>\" + \"\\n<|user|>:\" + question + \"</s>\" + \"\\n<|assistant|>:\"\n",
    "            inputs_embeds = torch.cat([soft_prompt.to(device), model.get_input_embeddings()(tokenizer(input_prompt, return_tensors=\"pt\").input_ids.to(device))], dim=1)\n",
    "            generate_kwargs = dict(\n",
    "                inputs_embeds=inputs_embeds,\n",
    "                streamer=streamer,\n",
    "                max_new_tokens=1024,\n",
    "                do_sample=True,\n",
    "                top_p=0.95,\n",
    "                top_k=50,\n",
    "                temperature=0.7,\n",
    "                num_beams=1,\n",
    "                stopping_criteria=StoppingCriteriaList([stop])\n",
    "            )\n",
    "            model.generate(**generate_kwargs)\n",
    "            output = \"\"\n",
    "            for new_token in streamer:\n",
    "                output += new_token\n",
    "                if '</s>' in output:\n",
    "                    break\n",
    "            true_ans = float(extract_answer(answer))\n",
    "            # print(output)\n",
    "            ans = extract_answer(output)\n",
    "            if ans == \"[invalid]\":\n",
    "                loss = torch.tensor(1.0)\n",
    "            else:\n",
    "                ans = float(ans)\n",
    "                if ans == 0 and true_ans == 0:\n",
    "                    loss = 0\n",
    "                else:\n",
    "                    loss = torch.abs(torch.tensor(ans - true_ans)) / (torch.abs(torch.tensor(true_ans)) + torch.abs(torch.tensor(ans)))\n",
    "            total_loss += loss\n",
    "    return (total_loss / batch_size).detach()\n",
    "\n",
    "def compute_loss_batched(model, tokenizer, sys_prompt, soft_prompt, data, batchsize, device, use_tqdm=False):\n",
    "\n",
    "    n_data = len(data)\n",
    "    n_batches = n_data // batchsize\n",
    "    total_loss = 0\n",
    "    if use_tqdm:\n",
    "        pbar = tqdm(total=n_data, desc=f\"Computing loss on {device}\", ncols=80)\n",
    "    for i in range(n_batches):\n",
    "        batch_data = data[i*batchsize:(i+1)*batchsize]\n",
    "        questions = [datum['question'] for datum in batch_data]\n",
    "        # answers = [datum['answer'] for datum in batch_data]\n",
    "        stop = BatchedStopOnTokens()\n",
    "        input_prompts = [\"<|system|>:\" + sys_prompt + \"</s>\" + \"\\n<|user|:\" + question + \"</s>\" + \"\\n<|assistant|>:\" for question in questions]\n",
    "        # tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer.pad_token = tokenizer.bos_token\n",
    "        # adding softprompt to each embedded input\n",
    "        inputs_embeds = []\n",
    "        for j in range(len(input_prompts)):\n",
    "            inputs_embeds.append(torch.cat([soft_prompt.to(device), model.get_input_embeddings()(tokenizer(input_prompts[j], return_tensors=\"pt\", padding=False, truncation=True, max_length=512).input_ids.to(device))], dim=1))\n",
    "        # get the max length of the inputs_embeds\n",
    "        max_len = max([inputs_embed.size(1) for inputs_embed in inputs_embeds])\n",
    "        # padding each input_embeds to the max length with tokenizer.pad_token_id on the left\n",
    "        bos_token_embed = model.get_input_embeddings()(torch.tensor(tokenizer.bos_token_id, device=device)).unsqueeze(0).unsqueeze(0).to(device)\n",
    "        attention_mask = []\n",
    "        for j in range(len(inputs_embeds)):\n",
    "            padding_len = max_len - inputs_embeds[j].size(1)\n",
    "            padding = bos_token_embed.repeat(1, padding_len, 1)\n",
    "            inputs_embeds[j] = torch.cat([padding, inputs_embeds[j]], dim=1)\n",
    "            attention_mask.append(torch.cat([torch.zeros(padding_len, device=device), torch.ones(inputs_embeds[j].size(1) - padding_len, device=device)]).unsqueeze(0))\n",
    "            # inputs_embeds[j] = nn.functional.pad(inputs_embeds[j], (0, 0, max_len - inputs_embeds[j].size(1), 0), value=tokenizer.pad_token_id)\n",
    "            # print(inputs_embeds[j].size())\n",
    "        # transfer to tensor\n",
    "        inputs_embeds = torch.concatenate(inputs_embeds, dim=0)\n",
    "        attention_mask = torch.concatenate(attention_mask, dim=0)\n",
    "        # print(inputs_embeds)\n",
    "        # print(inputs_embeds.size())\n",
    "        # padding side is left\n",
    "        # tokenizer.padding_side = \"left\"\n",
    "        # pad the inputs_embeds to match the batch dimension\n",
    "        # inputs_embeds = tokenizer.pad(inputs_embeds, return_tensors=\"pt\", padding=True, max_length=512)\n",
    "        # copy softprompt to match the batch dimension\n",
    "        # soft_prompt = soft_prompt.repeat(len(input_prompts), 1, 1)\n",
    "        # inputs_embeds = torch.cat([soft_prompt.to(device), model.get_input_embeddings()(tokenizer(input_prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).input_ids.to(device))], dim=1)\n",
    "        generate_kwargs = dict(\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            max_new_tokens=1024,\n",
    "            do_sample=True,\n",
    "            top_p=0.95,\n",
    "            top_k=50,\n",
    "            temperature=0.7,\n",
    "            num_beams=1,\n",
    "            stopping_criteria=StoppingCriteriaList([stop]),\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        outputs = model.generate(**generate_kwargs)\n",
    "        for j in range(outputs.size(0)):\n",
    "            if tokenizer.eos_token_id in outputs[j]:\n",
    "                eos_index = (outputs[j] == tokenizer.eos_token_id).nonzero()[0].item()\n",
    "                outputs[j, eos_index+1:] = tokenizer.pad_token_id\n",
    "        output_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        # print('\\n'.join(output_texts))\n",
    "        for j, output in enumerate(output_texts):\n",
    "            true_ans = float(extract_answer(batch_data[j]['answer']))\n",
    "            ans = extract_answer(output)\n",
    "            if ans == \"[invalid]\":\n",
    "                loss = torch.tensor(1.0)\n",
    "            else:\n",
    "                try: # 防止逆天输出'33.333...'\n",
    "                    ans = float(ans) \n",
    "                except:\n",
    "                    total_loss += torch.tensor(1.0)\n",
    "                    continue\n",
    "                if ans == 0 and true_ans == 0:\n",
    "                    loss = torch.tensor(0.0)\n",
    "                else:\n",
    "                    loss = torch.abs(torch.tensor(ans - true_ans)) / (torch.abs(torch.tensor(true_ans)) + torch.abs(torch.tensor(ans)))\n",
    "            total_loss += loss\n",
    "        if use_tqdm:\n",
    "            pbar.update(batchsize)\n",
    "    if use_tqdm:\n",
    "        pbar.close()\n",
    "    return (total_loss / n_data).detach()\n",
    "\n",
    "\n",
    "def compute_loss_concurrent(models, tokenizers, sys_prompt, soft_prompt, data, batch_size=4, n_GPU=4, use_tqdm=False):\n",
    "\n",
    "    n_data = len(data)\n",
    "    batchsize_per_GPU = n_data // n_GPU\n",
    "\n",
    "    # n_data = len(data)\n",
    "    # n_batches = n_data // n_GPU\n",
    "    # if n_data % n_GPU != 0:\n",
    "    #     n_batches += 1\n",
    "    total_loss = 0\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        futures = []\n",
    "        for i in range(n_GPU):\n",
    "            futures.append(executor.submit(compute_loss_batched, models[i], tokenizers[i], sys_prompt, soft_prompt, data[i*batchsize_per_GPU:(i+1)*batchsize_per_GPU], batch_size, torch.device(f'cuda:{i}'), use_tqdm=(use_tqdm and i == 0)))\n",
    "        # futures = executor.map(lambda i: compute_loss(model, sys_prompt, soft_prompt, data[i*n_batches:(i+1)*n_batches]), range(batch_size))\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            total_loss += future.result()\n",
    "    return total_loss / n_GPU\n",
    "\n",
    "# def compute_loss_concurrent(models, tokenizers, sys_prompt, soft_prompt, data, batch_size=4):\n",
    "\n",
    "#     n_data = len(data)\n",
    "#     n_batches = n_data // batch_size\n",
    "#     if n_data % batch_size != 0:\n",
    "#         n_batches += 1\n",
    "#     total_loss = 0\n",
    "\n",
    "#     # Create a pool of processes\n",
    "#     with mp.Pool(processes=batch_size) as pool:\n",
    "#         results = []\n",
    "#         for i in range(batch_size):\n",
    "#             # Use the model that is already on the correct device\n",
    "#             model = models[i]\n",
    "#             tokenizer = tokenizers[i]\n",
    "#             batch_data = data[i*n_batches:(i+1)*n_batches]\n",
    "#             # batch_data = [datum.to(device) for datum in batch_data]\n",
    "#             soft_prompt = soft_prompt.to(device).detach()\n",
    "#             # Compute loss in a separate process\n",
    "#             result = pool.apply_async(compute_loss, (model, tokenizer, sys_prompt, soft_prompt, batch_data))\n",
    "#             results.append(result)\n",
    "\n",
    "#         # Get the results\n",
    "#         for result in results:\n",
    "#             total_loss += result.get()\n",
    "\n",
    "#     return total_loss / batch_size\n",
    "\n",
    "def compute_loss_concurrent_globaled_model(sys_prompt, soft_prompt, data, batch_size=4):\n",
    "\n",
    "    n_data = len(data)\n",
    "    n_batches = n_data // batch_size\n",
    "    if n_data % batch_size != 0:\n",
    "        n_batches += 1\n",
    "    total_loss = 0\n",
    "\n",
    "    # Create a pool of processes\n",
    "    with mp.Pool(processes=batch_size) as pool:\n",
    "        results = []\n",
    "        for i in range(batch_size):\n",
    "            # Use the model that is already on the correct device\n",
    "            batch_data = data[i*n_batches:(i+1)*n_batches]\n",
    "            # batch_data = [datum.to(device) for datum in batch_data]\n",
    "            soft_prompt = soft_prompt.to(device).detach()\n",
    "            # Compute loss in a separate process\n",
    "            result = pool.apply_async(compute_loss, (sys_prompt, soft_prompt, batch_data))\n",
    "            results.append(result)\n",
    "\n",
    "        # Get the results\n",
    "        for result in results:\n",
    "            total_loss += result.get()\n",
    "\n",
    "    return total_loss / batch_size\n",
    "\n",
    "# def soft_prompt_training_gsm8k(model, n_tokens, sys_prompt, soft_prompt_init, training_data):\n",
    "\n",
    "#     # Initialize the soft prompt\n",
    "#     if soft_prompt_init is None:\n",
    "#         soft_prompt = torch.randn(1, n_tokens, embedding_dim)\n",
    "#     else:\n",
    "#         soft_prompt = torch.tensor(soft_prompt_init).to(device)\n",
    "    \n",
    "#     for data in training_data:\n",
    "\n",
    "#         question, answer = data['question'], data['answer']\n",
    "#         # print('question:', question)\n",
    "#         # print('answer:', answer)\n",
    "#         streamer = TextIteratorStreamer(tokenizer, timeout=10., skip_prompt=True, skip_special_tokens=True)\n",
    "#         stop = StopOnTokens()\n",
    "\n",
    "#         # generate input prompts\n",
    "#         input_prompt = \"<|system|>:\" + sys_prompt + \"</s>\" + \"\\n<|user|>:\" + question + \"</s>\" + \"\\n<|assistant|>:\"\n",
    "#         # generate inputs_embeds\n",
    "#         # print('input_prompt:', input_prompt)\n",
    "#         # inputs_embeds = torch.cat([soft_prompt, model.get_input_embeddings()(tokenizer(input_prompt, return_tensors=\"pt\").input_ids)], dim=1)\n",
    "#         inputs_embeds = model.get_input_embeddings()(tokenizer(input_prompt, return_tensors=\"pt\").input_ids)\n",
    "#         # generate outputs\n",
    "#         generate_kwargs = dict(\n",
    "#             inputs_embeds=inputs_embeds,\n",
    "#             streamer=streamer,\n",
    "#             max_new_tokens=1024,\n",
    "#             do_sample=True,\n",
    "#             top_p=0.95,\n",
    "#             top_k=50,\n",
    "#             temperature=0.7,\n",
    "#             num_beams=1,\n",
    "#             stopping_criteria=StoppingCriteriaList([stop])\n",
    "#         )\n",
    "#         model.generate(**generate_kwargs)\n",
    "#         # check output\n",
    "#         output = \"\"\n",
    "#         for new_token in streamer:\n",
    "#             output += new_token\n",
    "#             if '</s>' in output:\n",
    "#                 break\n",
    "#         # print(output)\n",
    "#         # get answer digits by finding \"/n####\"\n",
    "#         true_ans = float(extract_answer(answer))\n",
    "#         ans = extract_answer(output)\n",
    "#         if ans == \"[invalid]\":\n",
    "#             loss = torch.tensor(1.0)\n",
    "#         else:\n",
    "#             ans = float(ans)\n",
    "#             if ans == 0 and true_ans == 0:\n",
    "#                 loss = 0\n",
    "#             else:\n",
    "#                 loss = torch.abs(torch.tensor(ans - true_ans)) / (torch.abs(torch.tensor(true_ans)) + torch.abs(torch.tensor(ans)))\n",
    "#         print('LLaMA ans:', ans)\n",
    "#         print('true ans:', true_ans)\n",
    "#         print('loss:', loss)\n",
    "\n",
    "#     return soft_prompt\n",
    "\n",
    "with open('/mnt/yutong/data/grade_school_math/data/test.jsonl', 'r') as f:\n",
    "    data_test = f.readlines()\n",
    "    data_test = [json.loads(d) for d in data_test]\n",
    "\n",
    "with open('/mnt/yutong/data/grade_school_math/data/train.jsonl', 'r') as f:\n",
    "    data_train = f.readlines()\n",
    "    data_train = [json.loads(d) for d in data_train]\n",
    "\n",
    "# n_tokens = 10\n",
    "# # sys_prompt = \"You are a math problem solving assistant. Given the question, you should use your best capability to solve it and provide the answer. After providing the solution, you must add a straightforward answer of format '\\n#### <number>' in the end of your response. The <number> should only contain a number and '\\n#### <number>' must match the following regular expression: '\\n#### (\\-?[0-9\\.\\,]+)'.\"\n",
    "# # sys_prompt = \"You are a math problem solving assistant. Given the question, you should use your best capability to solve it and provide the answer. After providing the solution, you must add a straightforward answer of format '\\n#### <number>' in the end of your response. The <number> should be pure number consists of '-', '0'-'9', '.', ',', WITHOUT any other tokens representing the answer's unit. Note that '26' is a valid <number> and '/$26' is not. For example, '\\n#### /$18' is NOT a valid answer and must be modified to '\\n#### 18'. NOTE: You've failed a lot of trials before, because of extra explantory words after the answer region '\\n#### <number>' or a wrong <number> syntax as '\\$24'. Please make sure the answer is the last token in your response.\"\n",
    "# sys_prompt = \"Solving the follwing math problem and response with '\\n#### <answer>' with <answer> substituted by the correct number in the very end:\\n\"\n",
    "# soft_prompt_init = torch.randn(1, n_tokens, embedding_dim).to(device)\n",
    "# soft_prompt_training_gsm8k(peft_model, n_tokens, sys_prompt, soft_prompt_init, data_test[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1319\n",
      "7473\n"
     ]
    }
   ],
   "source": [
    "print(len(data_test))\n",
    "print(len(data_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "n_tokens = 10\n",
    "sys_prompt = \"Solving the following math problem and respond with '\\n#### <answer>' with <answer> substituted by the correct number in the very end:\\n\"\n",
    "models = [copy.deepcopy(model).to(torch.device(f'cuda:{_}')) for _ in range(4)]\n",
    "\n",
    "# soft_prompt_init = torch.randn(1, n_tokens, embedding_dim, dtype=torch.float16).to(device)\n",
    "soft_prompt_init = model.get_input_embeddings()(tokenizer(\"1+1=?\", return_tensors=\"pt\").input_ids)\n",
    "tokenizers = [AutoTokenizer.from_pretrained(model_pth,torch_dtype=torch.float16,load_in_4bit=False,load_in_8bit=False) for _ in range(3)] + [tokenizer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "peft_model_id = \"llama_2_7b_lora_2/1_epoch_finetuning\"\n",
    "# peft_model_id = \"llama_2_7b_lora_3/5_epoch_finetuning\"\n",
    "# peft_model_id = \"llama_2_7b_lora_3/checkpoint-1000\"\n",
    "peft_models = [PeftModel.from_pretrained(models[i], peft_model_id, torch_dtype=torch.float16) for i in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "\n",
    "def zero_order_softprompt_tuning_twopoints(model, sys_prompt, soft_prompt, training_data, validation_data, batchsize, maxIters, learning_rate, variation_scale):\n",
    "    # get batched data\n",
    "    batched_data = [training_data[i:i+batchsize] for i in range(0, len(training_data), batchsize)]\n",
    "    # compute parameters in soft_prompt\n",
    "    dimension = soft_prompt.numel()\n",
    "    for i in range(maxIters):\n",
    "        # get initial loss\n",
    "        loss = compute_loss(model, sys_prompt, soft_prompt, batched_data[i])\n",
    "        # random variation in softprompt as uniform unit ball distribution\n",
    "        random_directions = torch.randn_like(soft_prompt)\n",
    "        random_directions = random_directions / torch.norm(random_directions)\n",
    "        random_variations = random_directions * variation_scale\n",
    "        # get variation sampling\n",
    "        soft_prompt_plus = soft_prompt + random_variations\n",
    "        loss_plus = compute_loss(model, sys_prompt, soft_prompt_plus, batched_data[i])\n",
    "        # get loss difference\n",
    "        loss_diff = loss_plus - loss\n",
    "        # compute zero-order gradient\n",
    "        gradient = (loss_diff / variation_scale * dimension) * random_directions\n",
    "        # update softprompt\n",
    "        soft_prompt = soft_prompt - learning_rate * gradient\n",
    "        # validation\n",
    "        validation_loss = compute_loss(model, sys_prompt, soft_prompt, validation_data)\n",
    "        print(f\"Iteration {i}: Validation loss {validation_loss}\")\n",
    "    return soft_prompt\n",
    "\n",
    "def zero_order_softprompt_tuning_twopoints_concurrent(models, tokenizers, sys_prompt, soft_prompt, training_data, validation_data, batchsize, maxIters, learning_rate, variation_scale):\n",
    "    # get batched data\n",
    "    batched_data = [training_data[i:i+batchsize] for i in range(0, len(training_data), batchsize)]\n",
    "    # compute parameters in soft_prompt\n",
    "    dimension = soft_prompt.numel()\n",
    "    for i in range(maxIters):\n",
    "        # get initial loss\n",
    "        loss = compute_loss_concurrent(models, tokenizers, sys_prompt, soft_prompt, batched_data[i], batch_size=batchsize//4, n_GPU=4)\n",
    "        print(f'loss:{loss}')\n",
    "        # random variation in softprompt as uniform unit ball distribution\n",
    "        random_directions = torch.randn_like(soft_prompt)\n",
    "        random_directions = random_directions / torch.norm(random_directions)\n",
    "        random_variations = random_directions * variation_scale\n",
    "        # get variation sampling\n",
    "        soft_prompt_plus = soft_prompt + random_variations\n",
    "        loss_plus = compute_loss_concurrent(models, tokenizers, sys_prompt, soft_prompt_plus, batched_data[i], batch_size=batchsize//4, n_GPU=4)\n",
    "        print(f'loss_plus:{loss_plus}')\n",
    "        # get loss difference\n",
    "        loss_diff = loss_plus - loss\n",
    "        # compute zero-order gradient\n",
    "        gradient = (loss_diff / variation_scale * (dimension * learning_rate)) * random_directions\n",
    "        # update softprompt\n",
    "        soft_prompt = soft_prompt - gradient\n",
    "        # validation\n",
    "        validation_loss = compute_loss_concurrent(models, tokenizers, sys_prompt, soft_prompt, validation_data, batch_size=batchsize//4, n_GPU=4)\n",
    "        print(f\"Iteration {i}: Validation loss {validation_loss}\")\n",
    "    return soft_prompt\n",
    "\n",
    "def zero_order_softprompt_tuning_twopoints_concurrent_globaled_model(sys_prompt, soft_prompt, training_data, validation_data, batchsize, maxIters, learning_rate, variation_scale):\n",
    "    # get batched data\n",
    "    batched_data = [training_data[i:i+batchsize] for i in range(0, len(training_data), batchsize)]\n",
    "    # compute parameters in soft_prompt\n",
    "    dimension = soft_prompt.numel()\n",
    "    for i in range(maxIters):\n",
    "        # get initial loss\n",
    "        loss = compute_loss_concurrent_globaled_model(sys_prompt, soft_prompt, batched_data[i], batch_size=4)\n",
    "        # random variation in softprompt as uniform unit ball distribution\n",
    "        random_directions = torch.randn_like(soft_prompt)\n",
    "        random_directions = random_directions / torch.norm(random_directions)\n",
    "        random_variations = random_directions * variation_scale\n",
    "        # get variation sampling\n",
    "        soft_prompt_plus = soft_prompt + random_variations\n",
    "        loss_plus = compute_loss_concurrent_globaled_model(sys_prompt, soft_prompt_plus, batched_data[i], batch_size=4)\n",
    "        # get loss difference\n",
    "        loss_diff = loss_plus - loss\n",
    "        # compute zero-order gradient\n",
    "        gradient = (loss_diff / variation_scale * dimension) * random_directions\n",
    "        # update softprompt\n",
    "        soft_prompt = soft_prompt - learning_rate * gradient\n",
    "        # validation\n",
    "        validation_loss = compute_loss_concurrent_globaled_model(sys_prompt, soft_prompt, validation_data, batch_size=4)\n",
    "        print(f\"Iteration {i}: Validation loss {validation_loss}\")\n",
    "    return soft_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing loss on cuda:0:  98%|██████████████▋| 320/328 [16:59<00:25,  3.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4157)\n"
     ]
    }
   ],
   "source": [
    "# from peft import PeftModel\n",
    "# peft_model_id = \"llama_2_7b_lora_2/1_epoch_finetuning\"\n",
    "\n",
    "# def init_worker():\n",
    "#     global model, tokenizer\n",
    "#     model = AutoModelForCausalLM.from_pretrained(model_pth,torch_dtype=torch.float16,load_in_4bit=False,load_in_8bit=False)\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(model_pth,torch_dtype=torch.float16,load_in_4bit=False,load_in_8bit=False)\n",
    "#     model = PeftModel.from_pretrained(model, peft_model_id, torch_dtype=torch.float16)\n",
    "# set random seed\n",
    "import torch\n",
    "torch.manual_seed(0)\n",
    "import random\n",
    "random.seed(0)\n",
    "\n",
    "# load softprompt\n",
    "soft_prompt_path = 'softprompt_tuning/softprompt_epoch_5.pt'\n",
    "soft_prompt = torch.load(soft_prompt_path)\n",
    "\n",
    "# loss = compute_loss_batched(peft_models[0], tokenizers[0], sys_prompt, soft_prompt, data_test[:1312], 2, torch.device('cuda:0'))\n",
    "loss = compute_loss_concurrent(peft_models, tokenizers, sys_prompt, soft_prompt, data_test[:1312], 32, 4, use_tqdm=True)\n",
    "# 进度条\n",
    "# loss = compute_loss_concurrent(peft_models, tokenizers, sys_prompt, soft_prompt, data_train[:7456], 32, 4, use_tqdm=True)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "llama_2_7b_lora_2/1_epoch_finetuning loss: 0.4157(test), 0.xxxx(train); acc: 0.1616(test)\n",
    "\n",
    "llama_2_7b_lora_3/5_epoch_finetuning loss: 0.2827(test), 0.2374(train); acc: 0.3148(test)\n",
    "\n",
    "softprompt_tuning/softprompt_epoch_1: 0.2891(test), 0.2336(train); acc: 0.2896(test)\n",
    "\n",
    "softprompt_tuning/softprompt_epoch_2: 0.2862(test), 0.xxxx(train); acc: 0.3041(test)\n",
    "\n",
    "softprompt_tuning/softprompt_epoch_3: 0.2906(test),\n",
    "\n",
    "softprompt_tuning/softprompt_epoch_4: 0.2845(test),\n",
    "\n",
    "softprompt_tuning/softprompt_epoch_5: 0.2846(test),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.manual_seed(0)\n",
    "import random\n",
    "random.seed(0)\n",
    "# zero_order_softprompt_tuning_twopoints_concurrent(peft_models, tokenizers, sys_prompt, soft_prompt_init, data_train[:320], data_test[50:82], batchsize=32, maxIters=10, learning_rate=1e-7, variation_scale=1e-2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
